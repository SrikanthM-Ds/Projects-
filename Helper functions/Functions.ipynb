{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_conn(user_id, password):\n",
    "    from sqlalchemy import create_engine\n",
    "    from urllib.parse import quote      \n",
    "    server = create_engine('''mysql+pymysql://{0}:{1}s@10.95.60.125:3306/ypre'''.format(user_id,'%') % quote(password))\n",
    "    return server\n",
    "def header(name):\n",
    "    colorstr = \"\"\"<h4><center>{}</center></h4>\"\"\".format(name)\n",
    "    display(Markdown(colorstr))\n",
    "\n",
    "def preprocess_cohort_data(df_):\n",
    "    print('Data preprocessing starts')\n",
    "    df=df_.copy()\n",
    "    df['due_month'] = pd.to_datetime(df['emi_due_date']).apply(lambda x: x.strftime(\"%Y%m\") if pd.isna(x)!=True else np.nan)\n",
    "    df['disbursed_month'] = pd.to_datetime(df['disbursed_date']).apply(lambda x: x.strftime(\"%Y%m\") if pd.isna(x)!=True else np.nan)\n",
    "    df['source_month'] = pd.to_datetime(df['createdOn']).apply(lambda x: x.strftime(\"%Y%m\") if pd.isna(x)!=True else np.nan)\n",
    "    df['ppmt'] = np.where(df['delay']<=0,1,0)\n",
    "    header('Null Summary')\n",
    "    null_smry=df[['due_month','disbursed_month','source_month','ppmt','delay']].isnull().sum().reset_index()\n",
    "    null_smry.columns=['Column Names','# of Null Rows']\n",
    "    display(null_smry)\n",
    "    print('End of data preprocessing')\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohort_fetch(start_date,end_date,sql_userid='',sql_password=''):\n",
    "    server=get_sql_conn(sql_userid, sql_password)\n",
    "    if(server==None):\n",
    "        return 400\n",
    "    query=f''' select \n",
    "    userid, loan_id, user_band,product_name,principal,installment_due,loan_number,installment_number,\n",
    "    total_paid,penalty_paid,unpaid_emi,state,delay,createdOn, disbursed_date, emi_due_date,emi_paid_date \n",
    "    from ypdynamic.yp_emi_data_tbl \n",
    "\n",
    "    where emi_due_date >= \"{start_date}\" and emi_due_date <\"{end_date}\" and loan_number=1 and installment_number=1\n",
    "   ;\n",
    "    '''\n",
    "    data=pd.read_sql(query, server)\n",
    "    print(f'Data Fetch completed. Shape of data fetch: {data.shape}')\n",
    "    server.dispose()\n",
    "    data=preprocess_cohort_data(data)\n",
    "    print(f'Final shape of data after preprocessing: {data.shape}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sync_loan_application_v1(df_, conn):\n",
    "    ''' This function takes userIds/cid & loanIds to fetch syncIds from yp.yp_user_sync_data based on loan_application logic.\n",
    "    This code runs for one batch.\n",
    "    Input: \n",
    "    userIds : list of customer ids, list\n",
    "    loanIds : list of loan ids, list\n",
    "    batch_size : Batch size, int >0 & should be in multiples of 10,100,1000,10000 and so on.....\n",
    "    conn= SQL create_engine statement, e.g. create_engine('-------conn_string----------') \n",
    "    \n",
    "    Output:\n",
    "    DataFrame containing userid & syncid.\n",
    "    '''\n",
    "    if(conn==None ):\n",
    "        print('Check connection string or length of userids/oanids passed.')\n",
    "        return 400\n",
    "    import datetime as dt\n",
    "    user_lst=list(df_.userid.values.tolist()) \n",
    "    loanId_lst=list(df_.loan_id.values.tolist())\n",
    "    sync_batch_query='''select cid as userid, max(syncId) as syncId,appType\n",
    "                from yp.yp_user_sync_data\n",
    "                where cid in {0} and source = 'loan_application' and sourceId in {1} \n",
    "                group by cid;'''  ## Query to fetch syncId based on loan_application logic\n",
    "    print(f'Starting Fetch Logic: Loan Application')\n",
    "    t1=dt.datetime.now()\n",
    "    temp_query=sync_batch_query.format(tuple(user_lst),tuple(loanId_lst))\n",
    "    temp_data= pd.read_sql(temp_query, conn)\n",
    "    temp_data['fetch_logic']='loan_application'\n",
    "    print('Fetch Done')\n",
    "    elapsed=dt.datetime.now()-t1\n",
    "    print(\"df shape: \" + str(temp_data.shape) + \"Took H:M:S - : %02d:%02d:%02d\" % (elapsed.seconds // 3600, elapsed.seconds // 60 % 60, elapsed.seconds % 60))\n",
    "    return temp_data\n",
    "\n",
    "def run_sync_7day_logic_v1(df_,conn):\n",
    "    ''' This function takes dataframe containig userid & disburse_date to fetch syncIds from yp.yp_user_sync_data based on Approximation_7d_lag_from_disb_date logic.\n",
    "    This code runs for one batch.\n",
    "    Input: \n",
    "    df_ : dataframe containig userid & disburse_date\n",
    "    batch_size : Batch size, int >0 & should be in multiples of 10,100,1000,10000 and so on.....\n",
    "    conn= SQL create_engine statement, e.g. create_engine('-------conn_string----------') \n",
    "    \n",
    "    Output:\n",
    "    DataFrame containing userid & syncid.\n",
    "    '''\n",
    "    if(conn==None):\n",
    "        return 400\n",
    "    import datetime as dt\n",
    "    df=df_.copy()\n",
    "    df.disbursed_date=df.disbursed_date.astype('str')    # Converting disbursed date into string format. This will be used to create a temp table for filteration of data\n",
    "    user_lst=list(df.userid.values.tolist())             \n",
    "    sync_7day_query='''with base_data as (select column_0 as cid, column_1 as disbursed_date from ( values {0} ) as temp) \n",
    "\n",
    "            select t1.cid as userid, max(t1.syncId) as syncId,appType\n",
    "            from yp.yp_user_sync_data t1\n",
    "            join base_data t2 on t1.cid in {1} and t1.cid = t2.cid and \n",
    "            t1.updatedAt>= DATE_ADD(t2.disbursed_date, INTERVAL -7 DAY) and t1.updatedAt<=t2.disbursed_date\n",
    "            and source!='session'\n",
    "            group by 1;'''                     # Query to fetch syncId based on 7 day logic \n",
    "\n",
    "    print(f'Starting Fetch Logic: Approximation_7d_lag_from_disb_date')\n",
    "    t1=dt.datetime.now()\n",
    "    temp_table_data=','.join(['row'+str(tuple(i)) for i in df[df.userid.isin(user_lst)][['userid','disbursed_date']].values])   # data creation for temp table (table created in select statement)\n",
    "    temp_query=sync_7day_query.format(temp_table_data,tuple(user_lst))\n",
    "    temp_data= pd.read_sql(temp_query, conn)\n",
    "    temp_data['fetch_logic']='Approximation_7d_lag_from_disb_date'\n",
    "    print('Fetch Done')\n",
    "    elapsed=dt.datetime.now()-t1\n",
    "    print(\"df shape: \" + str(temp_data.shape) + \"Took H:M:S - : %02d:%02d:%02d\" % (elapsed.seconds // 3600, elapsed.seconds // 60 % 60, elapsed.seconds % 60))\n",
    "    return temp_data\n",
    "             \n",
    "\n",
    "def fetch_sync_v1(df_,batch_size=10000,sql_userid='',sql_password=''):\n",
    "    import datetime as dt\n",
    "    ''' This function takes dataframe containig atleast userid,loanid & disburse_date columns to fetch syncIds from yp.yp_user_sync_data based on loan_application & 7days from disburse date logic.\n",
    "    This code runs in batchwise mode.\n",
    "    Input: \n",
    "    df_          : dataframe containig atleast userid,loanid & disburse_date columns\n",
    "    batch_size   : Batch size, int >0 & should be in multiples of 10,100,1000,10000 and so on.....\n",
    "    sql_userid   : SQL user_id, default ='' No params i.e. params stored in credential.txt\n",
    "    sql_password : SQL password, default ='' No params i.e. params stored in credential.txt\n",
    "    \n",
    "    Output:\n",
    "    DataFrame containing all existing columns along with syncid.\n",
    "    '''\n",
    "    req_cols=['userid','loan_id','disbursed_date']\n",
    "    header('-'*5+'Fetching SyncIds for Data'+'-'*5)\n",
    "    df=df_.copy()\n",
    "    server=get_sql_conn(sql_userid,sql_password)    # Getting SQL create_engine statement\n",
    "    if(server==None):\n",
    "        return 400\n",
    "    if(not all([i in df.columns for i in req_cols])):\n",
    "        header(' Column Error ')\n",
    "        print(f'Please make sure all required columns : {req_cols} are present in data & with same name.')\n",
    "        return 400\n",
    "    try:\n",
    "        final_data=pd.DataFrame()\n",
    "        user_lst=df.userid.values.tolist()\n",
    "       \n",
    "        shape_lst=get_shape_lst(len(user_lst),batch_size)  # getting shape list based on size of data & batch_run_size\n",
    "\n",
    "        for i in range(len(shape_lst)-1):\n",
    "     \n",
    "            header_left(f'Start : End - {shape_lst[i]}:{shape_lst[i+1]}')\n",
    "            sub_df=df[df.userid.isin(user_lst[shape_lst[i]:shape_lst[i+1]])][req_cols]\n",
    "           \n",
    "            sync_loanApp=run_sync_loan_application_v1(sub_df, conn=server)    # Fetching sync based on loan_application _logic\n",
    "\n",
    "            null_sync_df=sub_df[~sub_df.userid.isin(sync_loanApp.userid.tolist())]  \n",
    "              # filteration of data where sync is null\n",
    "            sync_7day=run_sync_7day_logic_v1(null_sync_df[['userid','disbursed_date']],conn=server) # Fetching sync based on Approximation_7d_lag_from_disb_date _logic\n",
    "    \n",
    "            sub_df=sub_df.merge(sync_loanApp.append(sync_7day),on='userid',how='left')\n",
    "            final_data=final_data.append(sub_df)\n",
    "            print(f'\\n{sub_df[sub_df.syncId.notnull()].shape[0]}/{sub_df.shape[0]} , {round(100*sub_df[sub_df.syncId.notnull()].shape[0]/sub_df.shape[0],2)}% syncIds found.\\n')\n",
    "            \n",
    "        print('End of Batch Execution.')\n",
    "    except Exception as e:\n",
    "        header('ERROR Occured')\n",
    "        print(f'Error : {e}')\n",
    "    if(final_data.shape[0]>0):\n",
    "        print(f'\\nTotal {final_data[final_data.syncId.notnull()].shape[0]}/{final_data.shape[0]},{round(100*final_data[final_data.syncId.notnull()].shape[0]/final_data.shape[0],2)} syncIds found.')\n",
    "        header('-'*5+'Fetch Sync Complete.'+'-'*5)\n",
    "        return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(start_idx, end_idx,data1):\n",
    "    sync_7day_query='''with base_data as (select column_0 as cid, column_1 as disbursed_date from ( values {0} ) as temp)\n",
    "\n",
    "    select t1.cid,max(t1.bureau_customer_id) bureau_customer_id\n",
    "    from ypre.bureau_customer as t1\n",
    "    join base_data as t2 on t1.cid=t2.cid\n",
    "    join ypre.bureau_cibil_header_segment as t3 on t1.bureau_customer_id = t3.bureau_customer_id \n",
    "    and t3.date_processed<=t2.disbursed_date\n",
    "    group by 1;'''\n",
    "\n",
    "    temp_table_data = ','.join(['row' + str(tuple(i)) for i in data1[start_idx:end_idx][['userid', 'disbursed_date']].values])  # data creation for temp table (table created in select statement)\n",
    "    temp_query = sync_7day_query.format(temp_table_data)\n",
    "    cust_id_data=pd.read_sql(temp_query,server)    \n",
    "    return cust_id_data\n",
    "\n",
    "def fetch_bureau_custid(data1,batch_size=10000): \n",
    "    '''\n",
    "    syncid_df=datframe containing syncids\n",
    "    tablename= table name from where data need to be fetched\n",
    "    '''\n",
    "    import datetime as dt\n",
    "    df = pd.DataFrame()\n",
    "    print('data fetch start')\n",
    "    t1=dt.datetime.now()\n",
    "\n",
    "    for i in range(0, int(data1.shape[0] / batch_size) + 1):\n",
    "        try:\n",
    "            t1=dt.datetime.now()\n",
    "            start = i * batch_size\n",
    "            end = (i+1) * batch_size \n",
    "            a=run_batch(start,end,data1)\n",
    "            df = pd.concat([df,a],axis=0)\n",
    "            print(i,dt.datetime.now()-t1)\n",
    "        except:\n",
    "            header('ERROR Occured')\n",
    "            print(f'Error : {e}') \n",
    "\n",
    "            \n",
    "    print('Fetch is done')\n",
    "    elapsed=dt.datetime.now()-t1\n",
    "    print(\"df shape: \" + str(df.shape) + \"Took H:M:S - : %02d:%02d:%02d\" % (elapsed.seconds // 3600, elapsed.seconds // 60 % 60, elapsed.seconds % 60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_score(start_idx, end_idx,data1):\n",
    "    score='''select bureau_customer_id, score_date, score, score_v3, createdOn from ypre.bureau_cibil_score_segment \n",
    "where bureau_customer_id in {0};'''  \n",
    "\n",
    "    scores=pd.read_sql(score.format(tuple(data1[start_idx:end_idx]['bureau_customer_id'])),server)    \n",
    "    return scores\n",
    "\n",
    "def fetch_cibil_score(data1,batch_size=100000): \n",
    "    '''\n",
    "    syncid_df=datframe containing syncids\n",
    "    tablename= table name from where data need to be fetched\n",
    "    '''\n",
    "    import datetime as dt\n",
    "    df = pd.DataFrame()\n",
    "    print('data fetch start')\n",
    "    t1=dt.datetime.now()\n",
    "\n",
    "    for i in range(0, int(data1.shape[0] / batch_size) + 1):\n",
    "        try:\n",
    "            t1=dt.datetime.now()\n",
    "            start = i * batch_size\n",
    "            end = (i+1) * batch_size \n",
    "            a=run_batch_score(start,end,data1)\n",
    "\n",
    "            df = pd.concat([df,a],axis=0)\n",
    "            print(i,dt.datetime.now()-t1)\n",
    "        except:\n",
    "            header('ERROR Occured')\n",
    "            print(f'Error : {e}') \n",
    "\n",
    "            \n",
    "    print('Fetch is done')\n",
    "    elapsed=dt.datetime.now()-t1\n",
    "    print(\"df shape: \" + str(df.shape) + \"Took H:M:S - : %02d:%02d:%02d\" % (elapsed.seconds // 3600, elapsed.seconds // 60 % 60, elapsed.seconds % 60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Distribution_ppmt_event(data,distribution_by='due_month'):\n",
    "    due=pd.DataFrame()\n",
    "    due['cx_count']=data.groupby(distribution_by)['cid'].agg('count')\n",
    "    due['ppmt']=data.groupby(distribution_by)['ppmt'].agg('sum')\n",
    "    due['ppmt_rate']=due['ppmt']/due['cx_count']\n",
    "    due['event']=data.groupby(distribution_by)['event'].agg('sum')\n",
    "    due['event_rate']=due['event']/due['cx_count']\n",
    "    due['avg_ticketsize']=data.groupby(distribution_by)['principal'].agg('mean')\n",
    "    return due\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collectionRates(ds,delay_column,dpd_list = list(range(0,16))): \n",
    "\n",
    "    '''\n",
    "    This function generate a dataframe with collection rate and cummulative collection rate at each delay values\n",
    "    param :\n",
    "        ds : data frame with delay/dpd column\n",
    "        delay_column : str , feature name for the dpd column\n",
    "        dpd_list : list, list for dpd values at which collection rates are needed\n",
    "    output :\n",
    "        dataframe\n",
    "    demo :\n",
    "        get_collectionRates(ds = base_pop,delay_column='delay',dpd_list = list(range(0,14)))\n",
    "    '''\n",
    "    \n",
    "    dpd_list.extend([21,30,60,90,120,180,240,360])\n",
    "    \n",
    "    try:\n",
    "        for i in dpd_list:\n",
    "            ds1 = ds.loc[ds[delay_column] == i]\n",
    "            ds2 = ds.loc[ds[delay_column] >= i]\n",
    "            ds3 = ds.loc[ds[delay_column] <= i]\n",
    "            \n",
    "            collection_rate = round(100*ds1.shape[0]/ds2.shape[0],2)               \n",
    "            cumt_rate = round((ds3.shape[0]/ds.shape[0])*100,2)\n",
    "            temp = pd.DataFrame([{'dpd':i,'Collection_rate':collection_rate,'Cumulative_collectionRate':cumt_rate}])\n",
    "            \n",
    "            if i == 0:\n",
    "                temp['Collection_rate'] = temp['Cumulative_collectionRate']\n",
    "                master_data = temp\n",
    "            else:\n",
    "                master_data = pd.concat([master_data,temp],axis=0)\n",
    "                \n",
    "            master_data['delta_gain'] = master_data['Cumulative_collectionRate'] - master_data['Cumulative_collectionRate'].shift(1)\n",
    "\n",
    "    except ZeroDivisionError as e:\n",
    "        print('Cx with delay values ' + str(i) + ' or more are not present, hence output is limited till available delay values')\n",
    "        pass\n",
    "            \n",
    "    return master_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_crif(start_idx, end_idx,data1):\n",
    "    \n",
    "    score='''select \n",
    "a.uid as cid\n",
    ", b.user_id as crif_bureau_customer_id\n",
    ", c.bureau_customer_id as cibil_bureau_customer_id\n",
    ", a.crif_score \n",
    ", a.cibil_score\n",
    ", a.cibil_score_v3\n",
    ", a.cibil_score_used\n",
    "from yp.yp_re_data as a\n",
    "join ypre.crif_user_initial_request as b  ON a.crif_table_id = b.user_id \n",
    "join ypre.bureau_cibil_request as c  ON a.uid = c.bureau_cibil_request_id\n",
    "where uid in {0};'''  \n",
    "\n",
    "    scores=pd.read_sql(score.format(tuple(data1[start_idx:end_idx]['cid'])),server)    \n",
    "    return scores\n",
    "\n",
    "def fetch_crif_score(data1,batch_size=10000): \n",
    "    '''\n",
    "    syncid_df=datframe containing syncids\n",
    "    tablename= table name from where data need to be fetched\n",
    "    '''\n",
    "    import datetime as dt\n",
    "    df = pd.DataFrame()\n",
    "    print('data fetch start')\n",
    "    t1=dt.datetime.now()\n",
    "\n",
    "    for i in range(0, int(data1.shape[0] / batch_size) + 1):\n",
    "        try:\n",
    "            t1=dt.datetime.now()\n",
    "            start = i * batch_size\n",
    "            end = (i+1) * batch_size \n",
    "            a=run_batch_crif(start,end,data1)\n",
    "\n",
    "            df = pd.concat([df,a],axis=0)\n",
    "            print(i,dt.datetime.now()-t1)\n",
    "           \n",
    "        except:\n",
    "            header('ERROR Occured')\n",
    "            print(f'Error : {e}') \n",
    "\n",
    "            \n",
    "    print('Fetch is done')\n",
    "    elapsed=dt.datetime.now()-t1\n",
    "    print(\"df shape: \" + str(df.shape) + \"Took H:M:S - : %02d:%02d:%02d\" % (elapsed.seconds // 3600, elapsed.seconds // 60 % 60, elapsed.seconds % 60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_woe_charts(woe_values,cols_list):\n",
    "    '''\n",
    "    This function create plots bin range vs Population DISTN and Event_rate\n",
    "    woe_values : dataframe of woe values\n",
    "    cols_list : list of features for which woe charts needed\n",
    "        \n",
    "    '''\n",
    "    import matplotlib.pyplot as plt\n",
    "    for i in cols_list:\n",
    "        \n",
    "        a=woe_values[woe_values.VAR_NAME==i].fillna('NAN')     \n",
    "        fig, ax = plt.subplots(figsize=(10,5))\n",
    "        chart=a.plot(use_index=True,kind='bar', x='Cuts', y='DIST_POP', ax=ax, color='b')\n",
    "        a.plot(use_index=True,x='Cuts',y='EVENT_RATE', ax=ax,secondary_y=True, color='r')\n",
    "        for p in ax.patches:\n",
    "            height = p.get_height() \n",
    "            ax.text(x = p.get_x()+(p.get_width()), y = height, s = height.round(decimals=2), ha = \"left\", color=\"black\")\n",
    "\n",
    "        for x,y in zip(range(len(a['Cuts'])), a[\"EVENT_RATE\"]):\n",
    "            plt.text(x, y, '{:.03}'.format(y))\n",
    "        ax.set_ylabel('Population Distribution',fontsize=12) \n",
    "        ax.set_xlabel('Bin Range',fontsize=12)     \n",
    "        plt.title(i,fontsize=15)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unzipe(file_path,file_types='csv'):\n",
    "    import zipfile\n",
    "    import pandas as pd\n",
    "    zf = zipfile.ZipFile(path)\n",
    "    filenames=list(zf.namelist())\n",
    "    return filenames\n",
    "\n",
    "#temp_df=pd.read_csv(zf.open('sql_creds - Copy.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cohort_and_score_fetch(data,dpd,df):\n",
    "    import datetime as dt\n",
    "    from sqlalchemy import create_engine\n",
    "    from urllib.parse import quote     \n",
    "    from dateutil import relativedelta \n",
    "    #server = create_engine('''mysql+pymysql://{0}:{1}s@10.95.60.192:3306/ypre'''.format(creds[0],'%') % quote(creds[1]))    \n",
    "    \n",
    "    df=df.drop_duplicates('cid',keep='last')\n",
    "    count=dict()\n",
    "    data=data[~(data.userid.isin(df[df.sc_status==0]['cid'].to_list()))] # excluding score where sc_status=0 from cohort\n",
    "    count['Total_cx']=data.shape[0]\n",
    "    data=data.replace(-9999,np.nan)\n",
    "    data=data[~(np.isnan(data.delay))]  # excluding cx where delay is PH\n",
    "    count['Cx_payment_available']=data.shape[0]    \n",
    "    df=df[df.sc_status!=0]      # excluding cx where sc_status is 0 from scores\n",
    "    count['CX_scores_available']=df.shape[0] \n",
    "    data['PPMT']=np.where(data.delay<=0,1,0)      # creating ppmt flag\n",
    "    count['PPMT_count']=data[data.PPMT==1].shape[0]\n",
    "    data=data.rename(columns={'userid':'cid'})  \n",
    "    df_score_payment=pd.merge(data,df,on='cid',how='inner')      #merging score and cohort df \n",
    "    count['CX_scores_&_payment_available']=df_score_payment.shape[0]\n",
    "    df_score_payment['target']=np.where(df_score_payment['delay']>=dpd,1,np.where(df_score_payment['delay']<=0,0,2)) #  1: event, 0:non_event, 2:grey\n",
    "    df_score_payment=df_score_payment[df_score_payment.target!=2]  # excluding grey cx\n",
    "    count['Cx_#_score&payment_avail_excl_grey']=df_score_payment.shape[0]\n",
    "    return df_score_payment,count\n",
    "   \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreband_mapper(df,score_range):\n",
    "    \"\"\"This function maps the scores to a bands based on the score range given\n",
    "    df : data with scores\n",
    "    score_range : score_range for bands starting from worst scores\"\"\"\n",
    "    df=df.copy()\n",
    "    scores=[float(i.split(\", \")[1][0:-1]) for i in score_range]\n",
    "    bands=[i for i in range(len(scores),0,-1)]\n",
    "    print(df['score'].max(),df['score'].min(),df['score'].quantile(0.25))\n",
    "    def mapper(x):\n",
    "        for i,j in zip(scores,bands):\n",
    "            if x<i:\n",
    "                return j\n",
    "    df['scoreband']=df['score'].apply(lambda x :mapper(x))\n",
    "   \n",
    "    return df\n",
    "\n",
    "def psi_value(expected,actual):\n",
    "    \"\"\"this function calculates the psi value\n",
    "    expected : counts of cx in bands during development\n",
    "    actual :counts of cx in bands current\"\"\"\n",
    "    psi_df=pd.DataFrame({'expected':expected,'actual':actual})\n",
    "    psi_df['expected_%'] = psi_df['expected']/psi_df['expected'].sum()\n",
    "    psi_df['actual_%'] = psi_df['actual']/psi_df['actual'].sum()\n",
    "    psi_df['psi'] = (psi_df['expected_%'] - psi_df['actual_%']) * np.log(psi_df['expected_%'] / psi_df['actual_%'])\n",
    "    psi =np.round(psi_df['psi'].sum(),4)\n",
    "    return psi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandwise_metrics(df,count,start,score_range,expected,sample_type,model_name,month):\n",
    "    \"\"\"\n",
    "    this function returns bandwise and overall performance for the given cohort data\n",
    "    \"\"\"\n",
    "    if month==None:\n",
    "        day_of_month = pd.to_datetime(start).day\n",
    "        w = (day_of_month - 1) // 7 + 1\n",
    "        current_month=pd.to_datetime(start).strftime('%b %y')+' W'+str(w)\n",
    "    else:\n",
    "        current_month=pd.to_datetime(start).strftime('%b %y')\n",
    "\n",
    "    expected=[float(i) for i in expected]\n",
    "    \n",
    "    base=pd.DataFrame({'Total_cx':df.groupby('scoreband').agg('count')['cid'],\n",
    "              'Event':df.groupby('scoreband')['target'].agg('sum'),\n",
    "              'PPMT':df.groupby('scoreband')['PPMT'].agg('sum')}).reset_index()\n",
    "    base=base.sort_values('scoreband',ascending=False)\n",
    "    print(base.shape, len(score_range))\n",
    "    print(base)\n",
    "    base['score_bin_range']=score_range\n",
    "    base['Cohort Time']=current_month\n",
    "    base['Model_name']=[model_name for i in range(base.shape[0])]\n",
    "    base['sample_type']=[sample_type for i in range(base.shape[0])]\n",
    "    \n",
    "\n",
    "    # ks_gini_calculation_bandwise\n",
    "    base['Non_Event']=base['Total_cx']-base['Event']\n",
    "    base['Cumulative_Non_Event']=base['Non_Event'].cumsum()\n",
    "    base['Cumulative_Event']=base['Event'].cumsum()\n",
    "    base['Population_%']=np.round((base['Total_cx']/base['Total_cx'].sum()),2)\n",
    "    base['Cumulative_Non_Event_%']=np.round((base['Cumulative_Non_Event']/base['Non_Event'].sum()),2)\n",
    "    base['Cumulative_Event_%']=np.round((base['Cumulative_Event']/base['Event'].sum()),2)\n",
    "    base['KS%']=np.round(abs(base['Cumulative_Non_Event_%']-base['Cumulative_Event_%']),2)\n",
    "    base['Event_rate_%']=np.round((base['Event']/base['Total_cx']),2)\n",
    "    base['PPMT_rate_%']=np.round(base['PPMT']/base['Total_cx'],2)\n",
    "    base[\"Gini%\"] =np.round((((base[\"Cumulative_Event_%\"]+(base[\"Cumulative_Event_%\"]).shift(1).fillna(0))/2) \\\n",
    "    *((base[\"Cumulative_Non_Event_%\"])-(base[\"Cumulative_Non_Event_%\"]).shift(1).fillna(0))),2)\n",
    "    \n",
    "    base=base[['Model_name','Cohort Time','sample_type','scoreband','Total_cx','Population_%','PPMT_rate_%','Event_rate_%','PPMT','Event','Non_Event','Cumulative_Non_Event','Cumulative_Event','Cumulative_Non_Event_%','Cumulative_Event_%','KS%',\"Gini%\",\"score_bin_range\"]]\n",
    "\n",
    "    #   overall ks_gini_calculation\n",
    "    model_metric=dict()\n",
    "    model_metric['Model_Name']=model_name\n",
    "    model_metric['Cohort']=current_month\n",
    "    model_metric['Sample_Type']=sample_type\n",
    "    model_metric['Total_cx']=count['Total_cx']\n",
    "    model_metric['Paybehav_avail']=count['Cx_payment_available']\n",
    "    model_metric['Score_avail']=count['CX_scores_available']\n",
    "    model_metric['both_paybehav_&_score_avail']=count['CX_scores_&_payment_available'] \n",
    "    model_metric['nongrey_paybhev_&_score_avail']=count['Cx_#_score&payment_avail_excl_grey']\n",
    "    model_metric['drop_in_cx']= np.round((1-(count['CX_scores_&_payment_available']/count['Total_cx'])),2)\n",
    "    model_metric['Gini%']=np.round((2*((base['Gini%'].sum()))-1),2)\n",
    "    model_metric['KS%']=np.round(max(base['KS%']),2)\n",
    "    model_metric['Roc_Auc%']=np.round((base['Gini%'].sum()),2)\n",
    "    model_metric['PSI']=psi_value(expected,base['Total_cx'])\n",
    "    model_metric['Event Rate%']=(base['Event'].sum()/base['Total_cx'].sum())\n",
    "    model_metric['PPMT Rate%']=(base['PPMT'].sum()/base['Total_cx'].sum())\n",
    "    model_metric['Event']=base.Event.sum() \n",
    "    model_metric['PSI_class']=np.where(model_metric['PSI']<0.1,0,np.where(model_metric['PSI']>=0.2,2,1))\n",
    "    model_metric['Drop_in_payment_availability%']= np.round((1-(count['Cx_payment_available']/count['Total_cx'])),2)\n",
    "    model_metric['Drop_in_score_availability%']=np.round((1-(count['CX_scores_available']/count['Total_cx'])),2)\n",
    "    model_metric['Drop_excl_grey%']=np.round((1-(count['Cx_#_score&payment_avail_excl_grey']/count['CX_scores_&_payment_available'])),2)\n",
    "    \n",
    "     \n",
    "    model_df=pd.DataFrame(model_metric,index=range(1))\n",
    "\n",
    "   \n",
    "     \n",
    "    return base,model_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def woe_bins_pop(df,cols_list):\n",
    "    \"\"\"the function calculates the woe bins population and event rate in df\"\"\"\n",
    "    data=pd.DataFrame()\n",
    "    for i in cols_list:\n",
    "        df1=pd.DataFrame(df[[i,'event']].groupby(i).agg('count')/df.shape[0])\n",
    "        df1['event_rate']=df[[i,'event']].groupby(i).agg('sum')/df['event'].sum()\n",
    "        df1.columns=['Total','Event_rate']\n",
    "        df1['var']=i\n",
    "        data=pd.concat([data,df1],axis=0)\n",
    "    data['woe']=data.index\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_psi(data,expected='expected', actual='actual'):\n",
    "    ''' data   -> Dataframe containing band dev_count & current_count.\n",
    "        expected   -> dev_count column name\n",
    "        actual -> current_count column name\n",
    "    '''\n",
    "    if(not isinstance(data,pd.DataFrame)) or (data.empty):\n",
    "        return -1\n",
    "    data['%expected'] = data[expected]/data[expected].sum()\n",
    "    data['%actual'] = data[actual]/data[actual].sum()\n",
    "    psi_df = data.copy()\n",
    "    psi_df['psi'] = (psi_df['%expected'] - psi_df['%actual']) * np.log(psi_df['%expected'] / psi_df['%actual'])\n",
    "    psi = psi_df['psi'].sum()\n",
    "    \n",
    "    return psi_df,psi\n",
    "def visualize_psi(df,psi,pair=\"Train - Test\"):\n",
    "    melted_df=pd.melt(df, id_vars =['band'], value_vars =['%expected', '%actual'],var_name='',value_name='Pop%')\n",
    "    melted_df['Pop%']=[round(i,1) for i in melted_df['Pop%']*100]\n",
    "    plt.figure(1, figsize = (8,5))\n",
    "    sns.set_style(\"whitegrid\", {\"grid.color\": \".7\", \"grid.linestyle\": \":\"})\n",
    "    chart= sns.barplot(data=melted_df,y='Pop%',x='band',hue='',palette='Paired') #Paired, rocket\n",
    "    ax = plt.gca()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend( loc='upper left')\n",
    "    plt.title(\"\"\"{0} Population Stability Index PSI - {1}\"\"\".format(pair,psi))\n",
    "    # chart.bar_label(chart.containers[0])\n",
    "    # chart.bar_label(chart.containers[1])\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Population%')\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f'{file_name}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def woe_bins_pop(df,cols_list,):\n",
    "    \"\"\"the function calculates the woe bins population and event rate in df\"\"\"\n",
    "    \n",
    "    data=pd.DataFrame()\n",
    "    for i in cols_list:\n",
    "        df1=pd.DataFrame(df[[i,'event']].groupby(i).agg('count')/df.shape[0])\n",
    "        df1.columns=['Total']\n",
    "        df1['var']=i\n",
    "        data=pd.concat([data,df1],axis=0)\n",
    "    data['woe']=data.index\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOM_woe_bins_pop(df,woe):\n",
    "    total_var_pop=pd.DataFrame()\n",
    "    col_list=df.columns.to_list()\n",
    "    col_list.remove('cid')\n",
    "    for var in col_list:\n",
    "        var_data=df[['cid',var]]\n",
    "        var_pop=pd.DataFrame()\n",
    "        \n",
    "        for m,k in zip([nov,dec,jan,feb,mar,apr,may,june],range(8)):\n",
    "            a=var_data[var_data.cid.isin(m.cid.to_list())]\n",
    "            df1=pd.DataFrame(a[[var,'cid']].groupby(var).agg('count')/a.shape[0]*100)\n",
    "            df1.columns=['month_'+str(k)]\n",
    "            var_pop=pd.concat([var_pop,df1],axis=1)\n",
    "            var_pop['WOE']=var_pop.index\n",
    "        bins=woe[woe.VAR_NAME==var].sort_values('WOE',ascending=True)[['WOE','Cuts']]\n",
    "        var_pop=pd.merge(var_pop,bins,on='WOE',how='left')\n",
    "        var_pop['var']=var\n",
    "        total_var_pop=pd.concat([total_var_pop,var_pop],axis=0)\n",
    "    return total_var_pop[['var','Cuts','WOE','month_0','month_1','month_2','month_3','month_4','month_5','month_6','month_7']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(d,poscutoff=0.65,negcutoff=-0.65):\n",
    "    v1=[]\n",
    "    v2=[]\n",
    "    corr=[]\n",
    "    for a in d.columns:\n",
    "        for b in d.drop(labels=a,axis=1).columns:\n",
    "            if d[a].corr(d[b])>poscutoff:\n",
    "                v1.append(a)\n",
    "                v2.append(b)\n",
    "                corr.append(d[a].corr(d[b]))\n",
    "            elif d[a].corr(d[b])<negcutoff:\n",
    "                v1.append(a)\n",
    "                v2.append(b)\n",
    "                corr.append(d[a].corr(d[b]))\n",
    "    df=pd.DataFrame({'v1':v1,'v2':v2,'corr':corr})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_correlation_by_IV(df,iv):\n",
    "    \"\"\" df = correlation dataframe having v1,v2 and correlation\n",
    "    iv : iv_details obtained from get_iv_woe_conversion\"\"\"\n",
    "    cols_to_drop=[]\n",
    "    for i in range(df.shape[0]):\n",
    "        a=float(iv_details[iv_details.VAR_NAME==df['v1'].loc[i]]['IV'].values)\n",
    "        b=float(iv_details[iv_details.VAR_NAME==df['v2'].loc[i]]['IV'].values)\n",
    "        if a>b :            \n",
    "            if i[1] not in cols_drop:\n",
    "                cols_drop.append(i[1])\n",
    "        elif i[0] not in cols_drop:\n",
    "            cols_drop.append(i[0]) \n",
    "    print('Number of features to drop :-',len(cols_to_drop))\n",
    "    return cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_correlation_by_IV_hrlper(df=corel_dict['Correl_vars'],iv):\n",
    "    cols_to_drop=[]\n",
    "    for i in df:\n",
    "        a=float(iv_details[iv_details.VAR_NAME==i[0]]['IV'].values)\n",
    "        b=float(iv_details[iv_details.VAR_NAME==i[1]]['IV'].values)\n",
    "        if a>b :            \n",
    "            if i[1] not in cols_drop:\n",
    "                cols_drop.append(i[1])\n",
    "        elif i[0] not in cols_drop:\n",
    "            cols_drop.append(i[0])            \n",
    "    return cols_to_drop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "6209b9ff55fbb8ac2244490389c18204ec5eca53750a40636b48eaae1101e508"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
