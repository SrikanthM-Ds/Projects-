{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic python functions for use\n",
    "\n",
    "as on 03-10-2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get mysql database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_conn(user_id, password):\n",
    "    from sqlalchemy import create_engine\n",
    "    from urllib.parse import quote      \n",
    "    server = create_engine('''mysql+pymysql://{0}:{1}s@10.95.60.125:3306/ypre'''.format(user_id,'%') % quote(password))\n",
    "    return server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip files and get file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unzipe(file_path,file_types='csv'):\n",
    "    import zipfile\n",
    "    import pandas as pd\n",
    "    zf = zipfile.ZipFile(path)\n",
    "    filenames=list(zf.namelist())\n",
    "    return filenames\n",
    "\n",
    "#temp_df=pd.read_csv(zf.open('sql_creds - Copy.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get reorder of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_columns(df,first_cols=['']):\n",
    "    '''\n",
    "    This function reorder columns in a dataset\n",
    "    param:\n",
    "        df:dataframe\n",
    "        first_cols: list of columns which should be at left most\n",
    "    returns :\n",
    "        dataframe with ordered columns based on first_cols list\n",
    "    demo :\n",
    "        df = reorder_columns(df, ['cid','ppmt'])\n",
    "    '''\n",
    "\n",
    "    last_cols = [col for col in df.columns if col not in first_cols]\n",
    "    df = df[first_cols+last_cols]\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dup_records(ds,key_var):\n",
    "    \"\"\"\n",
    "    This function returns duplicate records count\n",
    "    param :\n",
    "        ds : dataframe\n",
    "        key_var : str, Variable name based on which duplcation present\n",
    "    return :\n",
    "        dataframe with freq of values of key_var where occurances are greater than 1\n",
    "    demo :\n",
    "        dup_records_by_cid = Get_dup_records(ds,'cid')\n",
    "    \"\"\"\n",
    "    temp = ds.groupby([key_var]).agg({key_var:'count'}).rename(columns={key_var:'Freq'}).reset_index()\n",
    "    temp = temp[temp['Freq']>1]\n",
    "    print(\"Total Duplicate records:: \" +str(temp.shape[0]))\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write dataframe to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_tabs(df_list, sheet_list, file_name):\n",
    "    \"\"\"\n",
    "    This function export data to multiple sheets of excel\n",
    "    param:\n",
    "        df_list : list of dataframes\n",
    "        sheet_list : list of sheet name where data to exported\n",
    "        file_name : Export file name \n",
    "        \n",
    "    E.g :\n",
    "        # list of dataframes and sheet names\n",
    "        #dfs = [Req_tracker_asofdate, Req_tracker_agg]\n",
    "        #sheets = ['Flow_Trends','Flow_Aggregate_Data']    \n",
    "        #dfs_tabs(dfs, sheets, f\"test.xlsx\")\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    writer = pd.ExcelWriter(file_name,engine='xlsxwriter')   \n",
    "    for dataframe, sheet in zip(df_list, sheet_list):\n",
    "        dataframe.to_excel(writer, sheet_name=sheet, startrow=0 , startcol=0, index=False)   \n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get standard names of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standardize_col_names(ds):\n",
    "    \"\"\"\n",
    "    This function returns pythonic names for columns\n",
    "    \n",
    "    \"\"\"\n",
    "    ds.columns = ds.columns.str.lower()\n",
    "    ds.columns = [re.sub(r\"[^\\w\\s]\", '_', col) for col in ds.columns ]\n",
    "    ds.columns = ds.columns.str.replace('^ +| +$', '_')\n",
    "    ds.columns = ds.columns.str.replace('__', '_')\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove spaces in columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_join(df, columns,sep='_'):\n",
    "    from functools import reduce\n",
    "    \"\"\"\n",
    "    Concat multiple string columns\n",
    "    param:\n",
    "        df : dataframe\n",
    "        columns : list of col names\n",
    "        sep : seperator\n",
    "        \n",
    "    \"\"\"\n",
    "    assert len(columns) > 1\n",
    "    slist = [df[x].astype(str) for x in columns]\n",
    "    return reduce(lambda x, y: x + sep + y, slist[1:], slist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find number in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_number(text):\n",
    "    import re\n",
    "    '''\n",
    "    Get all digits from a str object\n",
    "    '''\n",
    "    num = re.findall(r'[0-9]+',text)\n",
    "    return \" \".join(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get loan tenure for each cx/record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_totLoanCnt_tenure_tagging(ds,tenure_month_thr = 3, loan_freq_thr = 3):\n",
    "    '''\n",
    "    This function provides the tag in a cohort for \n",
    "        a) short tenure / long tenure : refer tenureType column for short/long term \n",
    "        b) Total loan count : refer loanFreqBkt column for total loan \n",
    "        c) loan_number : refer loan_number_InCohort columns to get loan_number in cohort\n",
    "    \n",
    "    param:\n",
    "        ds : data frame containing userid, loan_id,disbursed_date,installment_number\n",
    "        tenure_month_thr : cutoff for saying short term \n",
    "        loan_freq_thr : cutoff for loan availed\n",
    "    '''\n",
    "    loanCnt_df = ds[['userid','loan_id','disbursed_date']].drop_duplicates().sort_values(by=['userid','loan_id','disbursed_date'],ascending=[True,True,True])\n",
    "    loanCnt_df['loan_number_InCohort'] = loanCnt_df.groupby(['userid'])['loan_id'].rank(ascending=True)\n",
    "    \n",
    "    tenureTag_df = ds[['userid','loan_id','installment_number']].groupby(['userid','loan_id']).agg({'installment_number':'max'}).reset_index().rename(columns = {'installment_number':'maxInstNum'})\n",
    "\n",
    "    ds = pd.merge(ds,loanCnt_df[['userid','loan_id','loan_number_InCohort']],on=['userid','loan_id'],how='left')\n",
    "    ds = pd.merge(ds,tenureTag_df,on=['userid','loan_id'],how='left')\n",
    "\n",
    "    ds['tenureType'] = np.where(ds['maxInstNum'] <=tenure_month_thr,\"ShortTerm\",\n",
    "                                    np.where(ds['maxInstNum'] >tenure_month_thr,\"LongTerm\",'Rest'))\n",
    "    \n",
    "    ds['loanFreqBkt'] = np.where(ds['loan_number_InCohort']<= loan_freq_thr, 'LoansUpto'+ str(loan_freq_thr),\n",
    "                                   np.where(ds['loan_number_InCohort']>loan_freq_thr, 'LoansGT'+str(loan_freq_thr),'Rest'))\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get monthly ppmt and event_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_ppmt_event_rate(ds):\n",
    "    \n",
    "    ds = ds.rename(columns={'userid':'cid'})\n",
    "    ppmt_distn = pd.pivot_table(ds[['ppmt','cid','due_month']],index=['due_month'],columns='ppmt',aggfunc='count',margins=True).reset_index()\n",
    "    ppmt_distn.columns = ['due_month','ppmt_0','ppmt_1','total_cx']\n",
    "    ppmt_distn = ppmt_distn.drop(['ppmt_0'],axis=1)\n",
    "    ppmt_distn = ppmt_distn.rename(columns={'ppmt_1':'ppmt'})\n",
    "\n",
    "\n",
    "    event_distn = pd.pivot_table(ds[['target','cid','due_month']],index=['due_month'],columns='target',aggfunc='count',margins=True).reset_index()\n",
    "    event_distn.columns = [''.join(col).strip() for col in event_distn.columns.values]\n",
    "    event_distn.columns = [col.replace('cid','') for col in event_distn.columns]\n",
    "    event_distn = event_distn.drop(['All'],axis=1)\n",
    "\n",
    "    df = pd.merge(ppmt_distn,event_distn,on=['due_month'],how='left')\n",
    "    df = df.fillna(0)\n",
    "    df['ppmt_rate'] = df['ppmt']/df['total_cx']\n",
    "    df['event_rate'] = df['event']/df['total_cx']\n",
    "    df['event_rate_excl_grey'] = df['event']/(df['total_cx']- df['rest'])\n",
    "    df = df.rename(columns={'rest':'grey_cx'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get distribution based on a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Distribution_ppmt_event(data,distribution_by='due_month',new_cols=None):\n",
    "    due=pd.DataFrame()\n",
    "    due['cx_count']=data.groupby(distribution_by)['cid'].agg('count')\n",
    "    due['ppmt']=data.groupby(distribution_by)['ppmt'].agg('sum')\n",
    "    due['ppmt_rate']=due['ppmt']/due['cx_count']\n",
    "    # due['avg_ticketsize']=np.round(data.groupby(distribution_by)['principalDue'].agg('mean'),2)\n",
    "    if new_cols!=None:\n",
    "        for var in new_cols:\n",
    "            due[var]=data.groupby(distribution_by)[var].agg('sum')\n",
    "            due[str(var)+\"_%\"]=due[var]/due['cx_count']\n",
    "            \n",
    "    return due"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get Woe charts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_woe_charts(woe_values,cols_list):\n",
    "    '''\n",
    "    This function create plots bin range vs Population DISTN and Event_rate\n",
    "    woe_values : dataframe of woe values\n",
    "    cols_list : list of features for which woe charts needed\n",
    "        \n",
    "    '''\n",
    "    import matplotlib.pyplot as plt\n",
    "    for i in cols_list:\n",
    "        \n",
    "        a=woe_values[woe_values.VAR_NAME==i].fillna('NAN')     \n",
    "        fig, ax = plt.subplots(figsize=(10,5))\n",
    "        chart=a.plot(use_index=True,kind='bar', x='Cuts', y='DIST_POP', ax=ax, color='skyblue')\n",
    "        a.plot(use_index=True,x='Cuts',y='EVENT_RATE', ax=ax,secondary_y=True, color='r')\n",
    "        for p in ax.patches:\n",
    "            height = p.get_height() \n",
    "            ax.text(x = p.get_x()+(p.get_width()), y = height, s = height.round(decimals=2), ha = \"left\", color=\"black\")\n",
    "\n",
    "        for x,y in zip(range(len(a['Cuts'])), a[\"EVENT_RATE\"]):\n",
    "            plt.text(x, y, '{:.03}'.format(y))\n",
    "        ax.set_ylabel('Population Distribution',fontsize=12) \n",
    "        ax.set_xlabel('Bin Range',fontsize=12)     \n",
    "        plt.title(i,fontsize=15)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To map score to band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreband_mapper(df,score_range):\n",
    "    \"\"\"This function maps the scores to a bands based on the score range given\n",
    "    df : data with scores\n",
    "    score_range : score_range for bands starting from worst scores\"\"\"\n",
    "    df=df.copy()\n",
    "    scores=[float(i.split(\", \")[1][0:-1]) for i in score_range]\n",
    "    bands=[i for i in range(len(scores),0,-1)]\n",
    "    print(df['score'].max(),df['score'].min(),df['score'].quantile(0.25))\n",
    "    def mapper(x):\n",
    "        for i,j in zip(scores,bands):\n",
    "            if x<i:\n",
    "                return j\n",
    "    df['scoreband']=df['score'].apply(lambda x :mapper(x))\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get psi value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi_value(expected,actual):\n",
    "    \"\"\"this function calculates the psi value\n",
    "    expected : counts of cx in bands during development\n",
    "    actual :counts of cx in bands current\"\"\"\n",
    "    psi_df=pd.DataFrame({'expected':expected,'actual':actual})\n",
    "    psi_df['expected_%'] = psi_df['expected']/psi_df['expected'].sum()\n",
    "    psi_df['actual_%'] = psi_df['actual']/psi_df['actual'].sum()\n",
    "    psi_df['psi'] = (psi_df['expected_%'] - psi_df['actual_%']) * np.log(psi_df['expected_%'] / psi_df['actual_%'])\n",
    "    psi =np.round(psi_df['psi'].sum(),4)\n",
    "    return psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_psi(df,psi,pair=\"Train - Test\"):\n",
    "    melted_df=pd.melt(df, id_vars =['band'], value_vars =['%expected', '%actual'],var_name='',value_name='Pop%')\n",
    "    melted_df['Pop%']=[round(i,1) for i in melted_df['Pop%']*100]\n",
    "    plt.figure(1, figsize = (8,5))\n",
    "    sns.set_style(\"whitegrid\", {\"grid.color\": \".7\", \"grid.linestyle\": \":\"})\n",
    "    chart= sns.barplot(data=melted_df,y='Pop%',x='band',hue='',palette='Paired') #Paired, rocket\n",
    "    ax = plt.gca()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend( loc='upper left')\n",
    "    plt.title(\"\"\"{0} Population Stability Index PSI - {1}\"\"\".format(pair,psi))\n",
    "    # chart.bar_label(chart.containers[0])\n",
    "    # chart.bar_label(chart.containers[1])\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Population%')\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f'{file_name}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get MOM woe bins population and event_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOM_woe_bins_pop(ds,woe_df,month_col,cols_ToDrop=[]):\n",
    "    pop_df=pd.DataFrame()\n",
    "    month_distribution=ds[[month_col]].groupby(month_col).size()\n",
    "    col_list=list(set(ds.columns)-set(cols_ToDrop))\n",
    "    for var in col_list:\n",
    "        \n",
    "        \n",
    "        temp=ds[[var,month_col]].groupby([month_col,var]).size()/month_distribution\n",
    "        temp_pivot=pd.pivot_table(temp.reset_index(),values=0,index=var,columns=month_col).reset_index()\n",
    "        temp_pivot=temp_pivot.rename(columns={var:'WOE'})\n",
    "        # temp_pivot['VAR']=features_post_fwd_fs[0]\n",
    "        temp_pivot.insert(0,'VAR',var)\n",
    "        temp_merged=woe_df[woe_df.VAR_NAME==var][['WOE','Cuts']].merge(temp_pivot,on='WOE')\n",
    "        \n",
    "        pop_df=pop_df.append(temp_merged)\n",
    "    return pop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get MOM pupulation % and event rate in different cohort or data(train ,test.OOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bins_pop_sample(df,target,cols_list,sample_type):\n",
    "    \"\"\"\n",
    "    this  function calculates the woe bins population and event rate for a df\n",
    "    \n",
    "    df : Woe replaced dataframe\n",
    "    target : target or event\n",
    "    cols_list : final model features\n",
    "    sample_type : one of train,test,oot etc\n",
    "    \n",
    "    demo:\n",
    "            output=bins_pop_sample(train,y_train,final_features_list,'Train')\n",
    "    \"\"\"\n",
    "    \n",
    "    df['event']=target\n",
    "    data=pd.DataFrame()\n",
    "    for i in cols_list:\n",
    "        df1=pd.DataFrame(df[[i,'event']].groupby(i).agg('count')/df.shape[0])\n",
    "        df1['event_rate']=df[[i,'event']].groupby(i).agg('sum')/df['event'].sum()\n",
    "        df1.columns=[str(sample_type)+'_POP%',str(sample_type)+'_Event_rate%']\n",
    "        df1['VAR_NAME']=i\n",
    "        data=pd.concat([data,df1],axis=0)\n",
    "    data['WOE']=data.index\n",
    "    return data[['VAR_NAME','WOE',str(sample_type)+'_POP%',str(sample_type)+'_Event_rate%']]\n",
    "\n",
    "def Woe_bins_summary_overall(df_tuple,target_tuple,sample_types,Woe_df,final_features):\n",
    "    \"\"\"\n",
    "    This function creates population and event_rate for all the samples and gives a overall dataframe\n",
    "    \n",
    "    df_tuple : tuple of sample dataframes (X_train,X_teat,OOt)\n",
    "    target_tuple : tuple of target of samples (y_train,y_test,OOT.event)\n",
    "    sample_types : list of samples ['Train, Test,OOT']\n",
    "    Woe_df : Woe dataframe from get_woe_conversion function (Cuts should be in data)\n",
    "    final_features : final model features\n",
    "    \n",
    "    Demo :\n",
    "            output = Woe_bins_summary_overall(df_tuple=(X_train3,X_test3,OOT1_data),target_tuple= (y_train,y_test,Woe_OOT1_data.event),\n",
    "                                                                             sample_types= ['train','test','oot'],Woe_df=Woe_values,final_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_dict=dict()\n",
    "    for i in range(len(sample_types)):\n",
    "        summary_dict[sample_types[i]]=bins_pop_sample(df=df_tuple[i],target=target_tuple[i],cols_list=final_features,sample_type=sample_types[i])\n",
    "        if i==1:\n",
    "            overall_df=pd.merge(summary_dict[sample_types[0]],summary_dict[sample_types[i]],on=['VAR_NAME','WOE'],how='left')\n",
    "        elif i>1 and i<=len(sample_types):\n",
    "            overall_df=pd.merge(overall_df,summary_dict[sample_types[i]],on=['VAR_NAME','WOE'],how='left')\n",
    "    Woe_df['WOE']=Woe_df['WOE'].apply(lambda x : str(np.round(x,5)))\n",
    "    overall_df['WOE']=overall_df['WOE'].apply(lambda x : str(np.round(x,5)))\n",
    "    final_df=pd.merge(Woe_df[['VAR_NAME','Cuts','WOE']],overall_df,on=['VAR_NAME','WOE'],how='right')\n",
    "    return final_df,overall_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation between variables version 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correl_vars(ds,cutoff=0.65, is_cor_mat_return=True):\n",
    "    \"\"\"\n",
    "    This functions gives pair var list which are correlated based on the cutoff \n",
    "    param:\n",
    "        ds : dataframe\n",
    "        cutoff : cutoff to choose correl level\n",
    "        is_cor_mat_return : True if correlation matrix to be return\n",
    "    returns :\n",
    "        dict with below features:\n",
    "        high_cor_var : pair wise list for the correlated varibales based on cutoffs\n",
    "    demo :\n",
    "        correl_dict = correl_vars(model_df)\n",
    "    \"\"\"\n",
    "    corr_mat = ds.corr() # correl matrix\n",
    "    \n",
    "    var1 = []; var2 = []\n",
    "    for i in range(len(corr_mat.columns)):\n",
    "        for j in range(len(corr_mat.index)):\n",
    "            if (abs(corr_mat.iloc[i,j]) > cutoff) & (i>j):\n",
    "                var1.append(corr_mat.columns[i]); var2.append(corr_mat.index[j])\n",
    "    \n",
    "    high_cor_var = list(zip(var1,var2)) # correls vars list\n",
    "    \n",
    "    # Getting VIF's\n",
    "    #inv_corr_mat = np.linalg.inv(corr_mat)\n",
    "    #vif = pd.DataFrame(np.diag(inv_corr_mat), index=ds.columns).reset_index().rename(columns={'index':'Parameter',0:'VIF'}).sort_values(by = ['VIF'],ascending=False, ignore_index=True)\n",
    "    \n",
    "    # Other way by using statsmodels package\n",
    "#     from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "#     from statsmodels.tools.tools import add_constant\n",
    "#     vif = pd.DataFrame([variance_inflation_factor(add_constant(ds).values, i) for i in range(add_constant(ds).shape[1])], \\\n",
    "#                          index=add_constant(ds).columns, columns=['VIF']).reset_index().rename(columns={'index':'Parameter'}).drop(index=0).sort_values(by = ['VIF'],ascending=False, ignore_index=True)\n",
    "    \n",
    "    if is_cor_mat_return :\n",
    "        correl_dict = {'correl_matrix':corr_mat, 'Correl_vars' : high_cor_var, 'vif':np.nan}\n",
    "        return correl_dict\n",
    "    else :\n",
    "        correl_dict = {'Correl_vars' : high_cor_var, 'vif':np.nan}\n",
    "        return correl_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation between variables version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(d,poscutoff,negcutoff):\n",
    "    'returns dataframe with correlated variables and correlation value'\n",
    "    v1=[]\n",
    "    v2=[]\n",
    "    corr=[]\n",
    "    for a in d.columns:\n",
    "        for b in d.drop(labels=a,axis=1).columns:\n",
    "            if d[a].corr(d[b])>poscutoff:\n",
    "                v1.append(a)\n",
    "                v2.append(b)\n",
    "                corr.append(d[a].corr(d[b]))\n",
    "            elif d[a].corr(d[b])<negcutoff:\n",
    "                v1.append(a)\n",
    "                v2.append(b)\n",
    "                corr.append(d[a].corr(d[b]))\n",
    "    df=pd.DataFrame({'v1':v1,'v2':v2,'corr':corr})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting one features from correlated pairs for version 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_correlation_by_IV_hrlper(df,iv):\n",
    "    \"\"\" function takes input as\n",
    "    df/correlation pairs : tuple of correlated vars\n",
    "    IV : Iv dataframe with iv values\n",
    "    return columns to drop with low IV\n",
    "    \"\"\"\n",
    "    cols_to_drop=[]\n",
    "    for i in df:\n",
    "        a=float(iv[iv.VAR_NAME==i[0]]['IV'].values)\n",
    "        b=float(iv[iv.VAR_NAME==i[1]]['IV'].values)\n",
    "        if a>b :            \n",
    "            if i[1] not in cols_to_drop:\n",
    "                cols_to_drop.append(i[1])\n",
    "        elif i[0] not in cols_to_drop:\n",
    "            cols_to_drop.append(i[0])            \n",
    "    return cols_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting features for correlation version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_correlation_by_IV(df,iv):\n",
    "    \"\"\" \n",
    "    df : correlation dataframe having v1,v2 and correlation\n",
    "    iv : iv_details obtained from get_iv_woe_conversion\n",
    "    returns : columns to drop with low iv\n",
    "    \"\"\"\n",
    "    cols_to_drop=[]\n",
    "    for i in range(df.shape[0]):\n",
    "        a=float(iv[iv.VAR_NAME==df['v1'].loc[i]]['IV'].values)\n",
    "        b=float(iv[iv.VAR_NAME==df['v2'].loc[i]]['IV'].values)\n",
    "        if a>b :            \n",
    "            if i[1] not in cols_drop:\n",
    "                cols_drop.append(i[1])\n",
    "        elif i[0] not in cols_drop:\n",
    "            cols_drop.append(i[0]) \n",
    "    print('Number of features to drop :-',len(cols_to_drop))\n",
    "    return cols_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOE and IV calculation Version 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iv_woe(data, target, bins=10):\n",
    "    \"\"\" input : data with target and name of target column\n",
    "    returns woe df and IV df\"\"\"\n",
    "    \n",
    "    IV_df = pd.DataFrame()\n",
    "    Woe_df=pd.DataFrame()\n",
    "    \n",
    "    #Extract Column Names\n",
    "    cols = data.columns\n",
    "    \n",
    "    #Run WOE and IV on all the independent variables\n",
    "    for ivars in cols[~cols.isin([target])]:\n",
    "        if (data[ivars].dtype.kind in 'bifc') and (len(np.unique(data[ivars]))>10):\n",
    "            binned_x = pd.qcut(data[ivars], bins,  duplicates='drop')\n",
    "            d0 = pd.DataFrame({'x': binned_x, 'y': data[target]})\n",
    "        else:\n",
    "            d0 = pd.DataFrame({'x': data[ivars], 'y': data[target]})\n",
    "        d = d0.groupby(\"x\", as_index=False).agg({\"y\": [\"count\", \"sum\"]})\n",
    "        d.columns = ['Cutoff', 'N', 'Events']\n",
    "        d['% of Events'] =np.maximum(d['Events'],1) / d['Events'].sum()\n",
    "        d['Non-Events'] = d['N'] - d['Events']\n",
    "        d['% of Non-Events'] = np.maximum(d['Non-Events'],1) / d['Non-Events'].sum()\n",
    "        d['WoE'] = np.log(d['% of Events']/d['% of Non-Events'])\n",
    "        d['IV'] = d['WoE'] * (d['% of Events'] - d['% of Non-Events'])\n",
    "        d['VAR']=ivars\n",
    "        Woe_df=pd.concat([Woe_df,d])\n",
    "        temp =pd.DataFrame({\"Variable\" : [ivars], \"IV\" : [d['IV'].sum()]}, columns = [\"Variable\", \"IV\"])\n",
    "        IV_df=pd.concat([IV_df,temp], axis=0)\n",
    "        \n",
    "    return IV_df,Woe_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woe and IV calculation version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating functions for monotonic binning, WOE and IV values corresponding to each variable\n",
    "\n",
    "\n",
    "# define a binning function\n",
    "def mono_bin(Y, X, n = 5, force_bin=2):\n",
    "\n",
    "    import pandas.core.algorithms as algos\n",
    "    from pandas import Series\n",
    "    import scipy.stats.stats as stats\n",
    "    import re\n",
    "    import traceback\n",
    "    import string\n",
    "   \n",
    "    '''\n",
    "    This function creates the woe bins for numerical features\n",
    "    param:\n",
    "        Y : dependent variable array\n",
    "        X : indenpendent variable arrary\n",
    "        n : count of max bin\n",
    "        force_bin = count of bins if no bins suggested\n",
    "    returns:\n",
    "        data frame with woe conversions\n",
    "       \n",
    "    demo :\n",
    "        out_df = mono_bin(model_df.event, model_df.drop(['event'],axis=1),n=6)\n",
    "       \n",
    "    '''\n",
    "   \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
    "    r = 0\n",
    "    while np.abs(r) < 1:\n",
    "        try:\n",
    "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
    "            d2 = d1.groupby('Bucket', as_index=True)\n",
    "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
    "            n = n - 1\n",
    "        except Exception as e:\n",
    "            n = n - 1\n",
    "\n",
    "    if len(d2) == 1:\n",
    "        n = force_bin        \n",
    "        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
    "        if len(np.unique(bins)) == 2:\n",
    "            bins = np.insert(bins, 0, 1)\n",
    "            bins[1] = bins[1]-(bins[1]/2)\n",
    "        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)})\n",
    "        d2 = d1.groupby('Bucket', as_index=True)\n",
    "   \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"MIN_VALUE\"] = d2.min().X\n",
    "    d3[\"MAX_VALUE\"] = d2.max().X\n",
    "    d3[\"COUNT\"] = d2.count().Y\n",
    "    d3[\"EVENT\"] = d2.sum().Y\n",
    "    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
    "    d3=d3.reset_index(drop=True)\n",
    "   \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "   \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "   \n",
    "    return(d3)\n",
    "\n",
    "def char_bin(Y, X):\n",
    "\n",
    "    '''\n",
    "    This function creates the woe bins for categorical features, each unique category considered as one bin\n",
    "    param :\n",
    "        Y : dependent variable array\n",
    "        X : indenpendent variable arrary\n",
    "    returns:\n",
    "        data frame with woe conversions\n",
    "    demo :\n",
    "        out_df = char_bin(model_df.event, model_df.drop(['event'],axis=1))\n",
    "    '''\n",
    "       \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]    \n",
    "    df2 = notmiss.groupby('X',as_index=True)\n",
    "   \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"COUNT\"] = df2.count().Y\n",
    "    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
    "    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
    "    d3[\"EVENT\"] = df2.sum().Y\n",
    "    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
    "   \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "   \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    d3 = d3.reset_index(drop=True)\n",
    "   \n",
    "    return(d3)\n",
    "\n",
    "def get_woeIV_conversion_v1(df1, target):\n",
    "\n",
    "    import pandas.core.algorithms as algos\n",
    "    from pandas import Series\n",
    "    import scipy.stats.stats as stats\n",
    "    import re\n",
    "    import traceback\n",
    "    import string\n",
    "   \n",
    "    '''\n",
    "    This function returns WOE values for the both categorical and numerical features by using char_bin and mono_bin funcions\n",
    "    param :\n",
    "        df1 : dataframe\n",
    "        target : array, event or target variable array\n",
    "    returns :\n",
    "        iv_df : dataframe with iv, woe values and bkts\n",
    "        iv : dataframe, variable level iv values\n",
    "    demo :\n",
    "        woe_df , iv_details = get_woeIV_conversion(model_df,model_df.event)\n",
    "   \n",
    "    '''\n",
    "   \n",
    "    stack = traceback.extract_stack()\n",
    "    filename, lineno, function_name, code = stack[-2] # -2 is default value for system, sometime needs to change it to -8\n",
    "    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
    "    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
    "   \n",
    "    x = df1.dtypes.index\n",
    "    count = -1\n",
    "   \n",
    "    for i in x:\n",
    "        if i.upper() not in (final.upper()):\n",
    "            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
    "                conv = mono_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i\n",
    "                count = count + 1\n",
    "            else:\n",
    "                conv = char_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i            \n",
    "                count = count + 1\n",
    "               \n",
    "            if count == 0:\n",
    "                iv_df = conv\n",
    "            else:\n",
    "                iv_df = iv_df.append(conv,ignore_index=True)\n",
    "   \n",
    "    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
    "    iv = iv.reset_index()\n",
    "    iv = iv.sort_values('IV',ascending=False)\n",
    "   \n",
    "    # iv['IV_bkt'] = np.where(iv['IV']==0,\"A) Eq 0\",\n",
    "    #                              np.where(iv['IV']<=0.01,\"B) 0-0.01\",\n",
    "    #                                       np.where(iv['IV']<=0.02,\"C) 0.01-0.02\",\n",
    "    #                                                np.where(iv['IV']<=0.03,\"D) 0.02-0.03\",\n",
    "    #                                                         np.where(iv['IV']<=0.05,\"E) 0.03-0.05\",\n",
    "    #                                                                  np.where(iv['IV']<=0.1,\"F) 0.05-0.1\",\n",
    "    #                                                                           np.where(iv['IV']<=0.2,\"G) 0.1-0.2\",\n",
    "    #                                                                                    np.where(iv['IV']<=0.3,\"H) 0.2-0.3\",\n",
    "    #                                                                                             np.where(iv['IV']<=0.5,\"I) 0.3-0.0.5\",\n",
    "    #                                                                                                      np.where(iv['IV']<=0.7,\"J) 0.5-0.7\",\n",
    "    #                                                                                                               np.where(iv['IV']<=1,\"K) 0.7-1.0\",\"L) GT 1\")))))))))))                                                                                                            \n",
    "   \n",
    "   \n",
    "    # iv['iv_PredPowerCategory'] = np.where(iv['IV']< 0.02,'Unpredictive',\n",
    "    #                               np.where(iv['IV']< 0.1,'Weak Predictor',\n",
    "    #                                       np.where(iv['IV']< 0.3,'Medium Predictor',\n",
    "    #                                               np.where( (iv['IV']>= 0.3 ) & (iv['IV']<= 0.5 ),'Strong Predictor',np.where(iv['IV']> 0.5,'Suspicious Predictor','Unpredictive')))))\n",
    "\n",
    "    iv['IV_bkt'] = iv['IV'].apply(lambda x : get_iv_bkts(x))\n",
    "    iv['iv_PredPowerCategory'] = iv['IV'].apply(lambda x : get_iv_predpower_category(x))\n",
    "    tot_pop =  iv_df.groupby(['VAR_NAME']).agg({'COUNT':sum}).rename(columns = {'COUNT':'POP'}).reset_index()\n",
    "\n",
    "    iv_df = iv_df.merge(tot_pop,on='VAR_NAME',how='left')\n",
    "    iv_df['DIST_POP'] = iv_df['COUNT']/iv_df['POP']\n",
    "    iv_df['Cuts'] = iv_df['MIN_VALUE'].map(str)+\"-\" + iv_df['MAX_VALUE'].map(str)\n",
    "    iv_df['Cuts'] = np.where(iv_df['Cuts'] =='nan-nan','NA',iv_df['Cuts'])\n",
    "    iv_df['MIN_VALUE_proxy'] = np.where(iv_df['MIN_VALUE'].isnull(),-999999999999,iv_df['MIN_VALUE'])\n",
    "    iv_df = iv_df.set_index('Cuts')\n",
    "    iv_df = iv_df.sort_values(['VAR_NAME','MIN_VALUE_proxy'], ascending= [True,True])\n",
    "    iv_df = iv_df.drop(['MIN_VALUE_proxy'],axis=1)\n",
    "    iv_df = iv_df[~((iv_df['DIST_POP']==0 ) & (iv_df['MIN_VALUE'].isnull()))] # dropping extra rows generated in case of null value if any\n",
    "   \n",
    "    return(iv_df,iv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing Raw data with Woe values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacing_variable_with_WOE(df1,woe,diff_cols,replacemet_type='WOE'):\n",
    "    \n",
    "    '''\n",
    "    This function coverts original dataset into converted woe values, output of get_woeIV_conversion or WOE_calculator\n",
    "    param:\n",
    "        df1 : base data a dataframe \n",
    "        woe : converted woe values dataframe out of get_woeIV_conversion or WOE_calculator function\n",
    "        diff_cols : list of variables which needs to drop\n",
    "    returns:\n",
    "        dataframe \n",
    "        \n",
    "    demo :\n",
    "        woe_converted_df = replacing_variable_with_WOE(df1= base_df,woe= woe_df,diff_cols= ['event'])\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ## Replacing the original DataFrame value with WOE values\n",
    "    df2 = df1.copy()\n",
    "    iv_df = woe\n",
    "    \n",
    "    transform_vars_list = df1.columns.difference(diff_cols)\n",
    "    # leave this value blank if you need replace the original column values\n",
    "    transform_prefix = 'new_' \n",
    "    \n",
    "    # Replacing variables with WOE values\n",
    "    for var in transform_vars_list:\n",
    "        small_df = iv_df[iv_df['VAR_NAME'] == var]\n",
    "        transform_dict = list(zip(small_df.MIN_VALUE,small_df.MAX_VALUE,small_df.WOE))\n",
    "        replace_cmd = ''\n",
    "        replace_cmd1 = ''\n",
    "        if small_df.MIN_VALUE.isnull().sum(axis=0) > 0:\n",
    "            if var in df1.select_dtypes('object').columns.tolist():\n",
    "                for i in transform_dict:\n",
    "                    if pd.isna(i[0]) == True or pd.isna(i[1]) == True:\n",
    "                        if pd.isna(i[2]) == False:\n",
    "                            if replacemet_type == 'WOE':\n",
    "                                replace_cmd = replace_cmd + str(i[2])                         \n",
    "                            else:\n",
    "                                replace_cmd = replace_cmd + 'np.nan'\n",
    "                    else:\n",
    "                        if replacemet_type  == 'WOE':\n",
    "                            replace_cmd1 = replace_cmd1 + str(i[2]) + str(' if x == \"') + str(i[0]) + '\" else '                            \n",
    "                        else:\n",
    "                            replace_cmd1 = replace_cmd1 + '\"[' + str(i[0]) + ']\"' + str(' if x == \"') + str(i[0]) + '\" else ('\n",
    "                            \n",
    "            else:\n",
    "                for i in transform_dict:\n",
    "                    if pd.isna(i[0]) == True or pd.isna(i[1]) == True:\n",
    "                        if pd.isna(i[2]) == False:\n",
    "                            if replacemet_type == 'WOE':\n",
    "                                replace_cmd = replace_cmd + str(i[2])\n",
    "                            else:\n",
    "                                replace_cmd = replace_cmd + 'np.nan'\n",
    "                    else:\n",
    "                        if replacemet_type == 'WOE':\n",
    "                            replace_cmd1 = replace_cmd1 + str(i[2]) + str(' if x >= ') + str(i[0]) + ' and ' + str(' x <= ') + str(i[1]) + ' else '\n",
    "                        else:\n",
    "                            replace_cmd1 = replace_cmd1 + '\"['+ str(i[0]) + ' - '  + str(i[1]) + ']\"'+ str(' if x >= ') + str(i[0]) + ' and ' + str(' x <= ') + str(i[1]) + ' else ('\n",
    "                            \n",
    "    \n",
    "        else:\n",
    "            if var in df1.select_dtypes('object').columns.tolist():\n",
    "                for i in transform_dict:\n",
    "                    if replacemet_type == 'WOE':\n",
    "                        replace_cmd1 = replace_cmd1 + str(i[2]) + str(' if x == \"') + str(i[0]) + '\" else '\n",
    "                        replace_cmd = replace_cmd + '0'\n",
    "                    else:\n",
    "                        replace_cmd1 = replace_cmd1 + '\"[' + str(i[0]) + ']\"' + str(' if x == \"') + str(i[0]) + '\" else ('\n",
    "                        replace_cmd = replace_cmd + '0'\n",
    "            else:\n",
    "                for i in transform_dict:\n",
    "                    if replacemet_type == 'WOE':\n",
    "                        replace_cmd1 = replace_cmd1 + str(i[2]) + str(' if x >= ') + str(i[0]) + ' and ' + str(' x <= ') + str(i[1]) + ' else '\n",
    "                        replace_cmd = replace_cmd + '0'\n",
    "                    else:\n",
    "                        replace_cmd1 = replace_cmd1 + '\"[' +str(i[0]) + ' - '  + str(i[1]) + ']\"' +str(' if x >= ') + str(i[0]) + ' and ' + str(' x <= ') + str(i[1]) + ' else ('\n",
    "                        replace_cmd = replace_cmd + '0'\n",
    "                        \n",
    "        if replacemet_type == 'WOE':\n",
    "            replace_cmd2 = replace_cmd1 + replace_cmd\n",
    "        else:\n",
    "            replace_cmd2 = replace_cmd1 + replace_cmd + ')' *  (small_df[small_df['MIN_VALUE'].notna()].shape[0])\n",
    "            \n",
    "        df2[var] = df2[var].apply(lambda x:eval(replace_cmd2))\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get Logistic Regression coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logitModelCoef(estimator,ds):    \n",
    "    \"\"\"\n",
    "    Gives the intercepts and coefficients along with variable names for logistic regression.\n",
    "    estimator : fitted model\n",
    "    ds :  data on which logistic model was fit i.e. X_train\n",
    "    \n",
    "    demo :\n",
    "        model_coef = get_logitModelCoef(model, X_train)\n",
    "        \n",
    "    \"\"\"\n",
    "    intercept = pd.DataFrame({'variable' : 'intercept', 'coefficient' : estimator.intercept_})\n",
    "    coefficient = pd.DataFrame({'variable' : ds.columns, 'coefficient' : estimator.coef_.transpose().flatten()})\n",
    "    coefficient = coefficient.reindex(coefficient.coefficient.abs().sort_values(ascending = False).index)\n",
    "    return(pd.concat([intercept,coefficient], axis = 0).reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_featureImportance(estimator, ds): \n",
    "    '''\n",
    "    Gives feature importance of model\n",
    "    param :\n",
    "        estimator : fitted model\n",
    "        ds : dataframe used to built model, equivalent to X_train\n",
    "    demo :\n",
    "        feature_importance_df = get_featureImportance(model, X_train)\n",
    "    '''\n",
    "    return pd.DataFrame({'feature_name':ds.columns,'importance_value':estimator.feature_importances_}).sort_values('importance_value',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get Shap of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SHAP_featureImportance(estimator,ds):\n",
    "    '''\n",
    "    This function provides the importance values fof features present in the traning set\n",
    "    param :\n",
    "        estimator : fitted model \n",
    "        ds : data.frame, tranning set excluding target features equivalent to X_train\n",
    "    returns : dataframe with feature importance and graph\n",
    "    e.g. :\n",
    "        feat_imp = get_shap_featureImportance(model,X_train)\n",
    "    \n",
    "    '''\n",
    "    import shap\n",
    "    shap_values = shap.TreeExplainer(estimator).shap_values(ds)\n",
    "    feature_imp = pd.DataFrame(list(zip(ds.columns, np.abs(shap_values)[0].mean(0),shap_values[0].sum(0))), columns=['feature_name', 'feature_importance','importance_direction'])\n",
    "    feature_imp['importance_direction'] = np.where(feature_imp['importance_direction']<0,'-','+')\n",
    "    feature_imp = feature_imp.iloc[(-np.abs(feature_imp['feature_importance'].values)).argsort()]\n",
    "    print(\"SHAP Summary Plot:- \",'\\n')\n",
    "    shap.summary_plot(shap_values, ds, plot_type=\"bar\",show=False)\n",
    "    plt.title(\"Top-20 Features SHAP Importance Bar Plot\")\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get univariant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_univariateSmry(modelBase, target=\"target\"):\n",
    "    \"\"\"\n",
    "    This function creates the basic univariate summary for the model base\n",
    "    \"\"\"\n",
    "    var_start_list = pd.DataFrame(modelBase.dtypes, index=None)\n",
    "\n",
    "    uniquecnt = modelBase.apply(pd.Series.nunique)\n",
    "    desc = modelBase.describe().transpose()\n",
    "    cor = modelBase.select_dtypes(include=[\"Int64\", \"float64\"]).apply(lambda x: x.corr(modelBase[target])) # watch out for other numeric data types\n",
    "    zeros = modelBase.apply(lambda x: (x[x == 0].shape[0]/x.shape[0]))\n",
    "    null = modelBase.apply(lambda x: (x[x.isnull()].shape[0]/x.shape[0]))\n",
    "\n",
    "    var_start_list = var_start_list.merge(pd.DataFrame(uniquecnt), how=\"left\", left_index=True, right_index=True)\n",
    "    var_start_list.rename(columns={\"0_x\": \"type\", \"0_y\": \"var_vals\"}, inplace=True)\n",
    "\n",
    "    var_start_list = var_start_list.merge(desc[[\"min\", \"max\", \"mean\", \"50%\"]], how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "    var_start_list = var_start_list.merge(pd.DataFrame(cor), how=\"left\", left_index=True,\n",
    "    right_index=True)\n",
    "\n",
    "    var_start_list = var_start_list.merge(pd.DataFrame(zeros), how=\"left\", left_index=True, right_index=True)\n",
    "    var_start_list = var_start_list.merge(pd.DataFrame(null), how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "    var_start_list.rename(columns = {0: \"percentNull\", \"0_x\": \"CorrelationWithTarget\",\"0_y\": \"percentZeros\" , \"min\": \"var_min\",\n",
    "    \"max\": \"var_max\", \"50%\": \"var_median\", \"mean\": \"var_mean\"}, inplace=True)\n",
    "\n",
    "    return var_start_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill rate calculation Version 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillrate(df):\n",
    "    data=pd.DataFrame(columns=['variable','unique_values','count','mean','median','mode','max','min',\n",
    "                               'fill rate'])\n",
    "    for var in df.columns:\n",
    "        if ((df[var].dtype!='object') and (df[var].dtype!='datetime64[ns]')):\n",
    "            fill_rate=np.round((1-(df[var].isnull().sum()/len(df[var])))*100,3)\n",
    "            data=data.append({'variable':var,'unique_values':len(df[var].value_counts()),'count':len(df[var]),\n",
    "                         'mean':np.round(df[var].mean(),3),'median':np.round(df[var].median(),3),'mode':np.NAN,'max':df[var].max(),\n",
    "                          'min':df[var].min(),'fill rate':fill_rate},ignore_index=True)\n",
    "        else:\n",
    "            fill_rate=np.round((1-(df[var].isnull().sum()/len(df[var])))*100,3)\n",
    "            data=data.append({'variable':var,'unique_values':len(df[var].value_counts()),'count':len(df[var]),\n",
    "                         'mean':np.NAN,'median':np.NAN,'mode':df[var].mode()[0],'max':np.NAN,\n",
    "                          'min':np.NAN,'fill rate':fill_rate},ignore_index=True)\n",
    "                        \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fillrate or basichealth check version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basicHealthCheck(ds,place_holders =[-9999] ,is_quantile_smry = False, is_avrg_skew_kurtosis = False):\n",
    "    '''\n",
    "    This function calculate the basic description of data, useful for coverage analysis\n",
    "    param : \n",
    "        ds : dataframe\n",
    "        place_holders : list of values which to be excluded for coverage calculation\n",
    "        is_quantile_smry : bool, True if quantile distribution needed\n",
    "        is_avrg_skew_kurtosis : bool, True if skewness and kurtosis needed\n",
    "        \n",
    "    return :\n",
    "        dataframe with relevant field\n",
    "        \n",
    "    demo :\n",
    "        coverage_smry = get_basicHealthCheck(model_df, place_holders = [-9999,-1])\n",
    "    \n",
    "    '''\n",
    "    \n",
    "#     Basic metrics\n",
    "    d2 = pd.DataFrame(ds.isna().sum(),columns = ['nullCount'])\n",
    "    d2['dataType'] = d2.index.map(ds.dtypes)\n",
    "    d2['blankCount'] = d2.index.map((ds=='').sum())\n",
    "    d2['nonNullCount'] = d2.index.map(ds.notna().sum())\n",
    "    d2['uniqueCount'] = d2.index.map(ds.nunique())\n",
    "    d2['min'] = ds.min(numeric_only=True)\n",
    "    d2['max'] = ds.max(numeric_only=True)\n",
    "\n",
    "    d2['placeHolderCount']= ds[ds.isin(place_holders)].count()\n",
    "    d2['coveragePerc'] = round(1 - (d2['blankCount'] + d2['nullCount']+d2['placeHolderCount'])/ds.shape[0],3)\n",
    "    \n",
    "    if is_avrg_skew_kurtosis:\n",
    "\n",
    "        d2['mean'] = ds.mean()\n",
    "        d2['nonZeroMean'] = ds.replace(0, np.nan).mean()\n",
    "        d2['total']= ds.sum(numeric_only=True)\n",
    "        d2['std'] = ds.std()\n",
    "        d2['skewness'] = ds.skew()\n",
    "        d2['kurtosis'] = ds.kurtosis()\n",
    "        \n",
    "    \n",
    "    d2.reset_index(inplace=True)\n",
    "        \n",
    "    if is_quantile_smry:\n",
    "        \n",
    "#       Quantile distn:\n",
    "        quantileCuts = [0.05,0.1,0.25,0.5,0.75,0.8, 0.9, 0.95,0.98,0.99]\n",
    "        d1 = ds.quantile(quantileCuts).T\n",
    "        d1.reset_index(inplace=True)\n",
    "        qNames = [f'Q{int(x* 100)}' for x in quantileCuts]\n",
    "        newNames = ['index']\n",
    "        newNames.extend(qNames)\n",
    "        d1.columns = newNames  \n",
    "        \n",
    "        smry = d2.merge(d1, on='index', how='left')\n",
    "        \n",
    "    else :\n",
    "        \n",
    "        smry = d2\n",
    "    \n",
    "#     creating master summary\n",
    "    \n",
    "    smry.rename(columns={\"index\":\"parameterName\"},inplace=True)\n",
    "    \n",
    "#     re-arranging columns\n",
    "    col_list = ['parameterName','dataType','coveragePerc','nullCount','blankCount','uniqueCount','placeHolderCount','nonNullCount','min','max']\n",
    "    smry = smry[col_list]\n",
    "    \n",
    "    return smry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ks table and model summary version 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks(data=None,target=None, prob=None):\n",
    "    data['target0'] = 1 - data[target]\n",
    "    data['bucket'] = pd.qcut(data[prob], 10,duplicates='drop')\n",
    "    grouped = data.groupby('bucket', as_index = False)\n",
    "    kstable = pd.DataFrame()\n",
    "    kstable['min_prob'] = grouped.min()[prob]\n",
    "    kstable['max_prob'] = grouped.max()[prob]\n",
    "    kstable['events']   = grouped.sum()[target]\n",
    "    kstable['nonevents'] = grouped.sum()['target0']\n",
    "    kstable = kstable.sort_values(by=\"min_prob\", ascending=False).reset_index(drop = True)\n",
    "    kstable['event_rate'] = (kstable.events / data[target].sum()).apply('{0:.2%}'.format)\n",
    "    kstable['nonevent_rate'] = (kstable.nonevents / data['target0'].sum()).apply('{0:.2%}'.format)\n",
    "    kstable['cum_eventrate']=(kstable.events / data[target].sum()).cumsum()\n",
    "    kstable['cum_noneventrate']=(kstable.nonevents / data['target0'].sum()).cumsum()\n",
    "    kstable['KS'] = np.round(kstable['cum_eventrate']-kstable['cum_noneventrate'], 3) * 100\n",
    "\n",
    "    #Formating\n",
    "    kstable['cum_eventrate']= kstable['cum_eventrate'].apply('{0:.2%}'.format)\n",
    "    kstable['cum_noneventrate']= kstable['cum_noneventrate'].apply('{0:.2%}'.format)\n",
    "\n",
    "    kstable.index = range(1,11)\n",
    "    kstable.index.rename('Decile', inplace=True)\n",
    "    pd.set_option('display.max_columns', 9)\n",
    "    ks=kstable.KS.max()\n",
    "    return kstable,ks\n",
    "def scorecard(model,X_train,X_test,y_test):\n",
    "    y_pred=model.predict(X_test)\n",
    "    y_prob=model.predict_proba(X_test)[:,1]\n",
    "    cc=confusion_matrix(y_test,y_pred)\n",
    "    TN=cc[0,0]\n",
    "    TP=cc[1,1]\n",
    "    FP=cc[0,1]\n",
    "    FN=cc[1,0]\n",
    "    accuracy=(TP+TN)/(TP+TN+FP+FN)\n",
    "    precision=TP/(TP+FP)\n",
    "    recall=TP/(TP+FN)\n",
    "    Tpr=TP/(TP+FN)\n",
    "    Fpr=FP/(FP+TN)\n",
    "    f1_ratio=2*((recall*precision)/(precision+recall))\n",
    "    auc_test=roc_auc_score(y_test,y_pred)\n",
    "    data=pd.DataFrame({'y':y_test,'p':y_prob})\n",
    "    a,ks_test=ks(data=data,target='y',prob='p')\n",
    "    y_pred_tr=model.predict(X_train)\n",
    "    y_prob_tr=model.predict_proba(X_train)[:,1]\n",
    "    cctrain=confusion_matrix(y_train,y_pred_tr)\n",
    "    TN=cctrain[0,0]\n",
    "    TP=cctrain[1,1]\n",
    "    FP=cctrain[0,1]\n",
    "    FN=cctrain[1,0]\n",
    "    accuracy_tr=(TP+TN)/(TP+TN+FP+FN)\n",
    "    precision_tr=TP/(TP+FP)\n",
    "    recall_tr=TP/(TP+FN)\n",
    "    Tpr_tr=TP/(TP+FN)\n",
    "    Fpr_tr=FP/(FP+TN)\n",
    "    f1_ratio_tr=2*((recall_tr*precision_tr)/(precision_tr+recall_tr))\n",
    "    auc_tr=roc_auc_score(y_train,y_pred_tr)\n",
    "    data_r=pd.DataFrame({'y':y_train,'p':y_prob_tr})\n",
    "    a,ks_train=ks(data=data_r,target='y',prob='p')\n",
    "    df=pd.DataFrame({'metrics':['accuracy','precision','recall','TPR','FPR','f1_ratio','auc_score','ks statistics'],\n",
    "                     'train':[accuracy_tr,precision_tr,recall_tr,Tpr_tr,Fpr_tr,f1_ratio_tr,auc_tr,ks_train],\n",
    "                      'test':[accuracy,precision,recall,Tpr,Fpr,f1_ratio,auc_test,ks_test]})\n",
    "    df=df.set_index('metrics')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get model summary version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba_score_performance(actual, prediction, test_set=False, train_bins=0, event=1, target=\"target\",\n",
    "                            probability=\"Probability_of_event\", event_name=\"Event\", total_name=\"Total\",ascending=False, bins=10):\n",
    "    \"\"\"\n",
    "    Get the KS/Gini coefficient and the table to create the lorenz curve with 10 bins\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual: pd.Series\n",
    "    A pandas Series with the target values\n",
    "    prediction: np.array\n",
    "    A numpy array with the predicted probabilities or score. 1 D array with the same length as actual\n",
    "    test_set: bool\n",
    "    Set to False if the prediction needs to be binned using quantiles. True if training set bins are present\n",
    "    train_bins = a list of cut points if this is True\n",
    "    train_bins: list\n",
    "    list of cutpoints that bin the training set into 10 parts\n",
    "    event: integer\n",
    "    The target value the gini table needs to be created for\n",
    "    target: str\n",
    "    The name of the target column in `actual`. If the name does not match, it will be changed to the user input\n",
    "    probability: str\n",
    "    column name of the column that contains the band\n",
    "    event_name: str\n",
    "    column name of the column that contains the event count for every band\n",
    "    total_name: str\n",
    "    column name of the column that contains the total count for every band\n",
    "    ascending: bool\n",
    "    Order of the probability or score band in the final table\n",
    "    bins: integer\n",
    "    no. of quantile bins to create\n",
    "    \"\"\"\n",
    "    actual.name = target\n",
    "    performance = pd.concat([pd.DataFrame(prediction, columns=[probability], index=actual.index), pd.DataFrame(actual)], axis=1)\n",
    "    performance.loc[:, target] = np.where(performance.loc[:, target] == event, 1, 0)\n",
    "\n",
    "    if test_set:\n",
    "        performance[probability] = pd.cut(performance.loc[:, probability], bins=train_bins, include_lowest=True)\n",
    "    else:\n",
    "        _, train_bins = pd.qcut(performance.loc[:, probability].round(12), bins, retbins=True, duplicates=\"drop\")\n",
    "        train_bins[0] = np.min([0.0, performance.loc[:, probability].min()])\n",
    "        train_bins[train_bins.shape[0]-1] = np.max([1.0, performance.loc[:, probability].max()])\n",
    "        performance[probability] = pd.cut(performance.loc[:, probability], bins=train_bins, include_lowest=True)\n",
    "\n",
    "    performance = pd.concat([performance.groupby(by=probability)[target].sum(),\n",
    "    performance.groupby(by=probability)[target].count()], axis=1)\n",
    "    performance[probability] = performance.index\n",
    "    performance.columns = [event_name, total_name, probability]\n",
    "\n",
    "    performance, model_KS, model_Gini = ks_gini_metrics(performance, probability=probability, event_name=event_name,\n",
    "    total_name=total_name, ascending=ascending)\n",
    "    \n",
    "    performance['Band'] = range(performance.shape[0]-1, -1, -1)\n",
    "\n",
    "    if test_set:\n",
    "        return performance, model_KS, model_Gini\n",
    "    else:\n",
    "        return performance, model_KS, model_Gini, train_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_gini_metrics(base, probability=\"Probability_of_event\", event_name=\"Event\", total_name=\"Total\",ascending=False):\n",
    "    \"\"\"\n",
    "    Get the KS/Gini coefficient from a pivot table with 3 specific columns - probability/score band, event count and total count\n",
    "    Parameters\n",
    "    ----------\n",
    "    base: pd.DataFrame\n",
    "    A pandas dataframe created using a group by operation with a probability or score band.\n",
    "    The pivot should be created with margins=False\n",
    "    probability: str\n",
    "    column name of the column that contains the band\n",
    "    event_name: str\n",
    "    column name of the column that contains the event count for every band\n",
    "    total_name: str\n",
    "    column name of the column that contains the total count for every band\n",
    "    ascending: bool\n",
    "    Order of the probability or score band in the final table\n",
    "    \"\"\"\n",
    "    base = base.loc[:, [probability, event_name, total_name]]\n",
    "    base = base[base.loc[:, total_name].notnull()]\n",
    "    base = base.append(pd.DataFrame(data={event_name: np.sum(base.loc[:, event_name]),\n",
    "    total_name: np.sum(base.loc[:, total_name]),\n",
    "    probability: \"All\"},index=[\"All\"]), ignore_index=True, sort=True)\n",
    "\n",
    "    base = base[base.loc[:, probability] != \"All\"]. \\\n",
    "    sort_values(by=probability, ascending=ascending). \\\n",
    "    append(base[base.loc[:, probability] == \"All\"], sort=True).loc[:, [probability, total_name, event_name]]\n",
    "\n",
    "    base[\"Non_\"+event_name] = base.loc[:, total_name] - base.loc[:, event_name]\n",
    "    base[\"Cumulative_Non_\"+event_name] = base.loc[:, \"Non_\"+event_name].cumsum()\n",
    "    base.loc[base[base.loc[:, probability] == \"All\"].index, \"Cumulative_Non_\"+event_name] = \\\n",
    "    base.loc[base[base.loc[:, probability] == \"All\"].index, \"Non_\"+event_name]\n",
    "    base[\"Cumulative_\"+event_name] = base.loc[:, event_name].cumsum()\n",
    "    base.loc[base[base.loc[:, probability] == \"All\"].index, \"Cumulative_Event\"] = \\\n",
    "    base.loc[base[base.loc[:, probability] == \"All\"].index, \"Event\"]\n",
    "    base[\"Population_%\"] = base.loc[:, total_name]/base[base.loc[:, probability] == \"All\"].loc[:, total_name].values\n",
    "    base[\"Cumulative_Non_\"+event_name+\"_%\"] = \\\n",
    "    base.loc[:, \"Cumulative_Non_\"+event_name]/base[base.loc[:, probability] == \"All\"].loc[:, \"Cumulative_Non_\"+event_name].values\n",
    "    base[\"Cumulative_\"+event_name+\"_%\"] = \\\n",
    "    base.loc[:, \"Cumulative_\"+event_name]/base[base.loc[:, probability] == \"All\"].loc[:, \"Cumulative_\"+event_name].values\n",
    "    base[\"KS_Stat\"] = base[\"Cumulative_\"+event_name+\"_%\"] - base[\"Cumulative_Non_\"+event_name+\"_%\"]\n",
    "    base[event_name+\"_rate\"] = base.loc[:, event_name]/base.loc[:, total_name]\n",
    "\n",
    "    base[\"Gini\"] = ((base[\"Cumulative_\"+event_name+\"_%\"]+base[\"Cumulative_\"+event_name+\"_%\"].shift(1).fillna(0))/2) \\\n",
    "    *(base[\"Cumulative_Non_\"+event_name+\"_%\"]-base[\"Cumulative_Non_\"+event_name+\"_%\"].shift(1).fillna(0))\n",
    "\n",
    "    base.loc[base[base.loc[:, probability] == \"All\"].index, \"Gini\"] = np.nan\n",
    "    model_KS = np.max(base[base.loc[:, probability] != \"All\"].KS_Stat)*100\n",
    "    model_Gini = (2*(np.sum(base[base.loc[:, probability] != \"All\"].Gini))-1)*100\n",
    "    return base, model_KS, model_Gini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perf_metric(estimator, X_train, X_test,y_train, y_test ,th = 0.5,bins=10):\n",
    "    \"\"\"\n",
    "    This function gives various performance metrics of train and test set for binary classification.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance; Trained classifier.\n",
    "    X_train : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    Input values for training set.\n",
    "    X_test : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    Input values for testing set.\n",
    "    y_train : array-like of shape (n_samples,)\n",
    "    Target values for training set.\n",
    "    y_test : array-like of shape (n_samples,)\n",
    "    Target values for testing set.\n",
    "    th : threshold above which predicted probability are tagged as class 1\n",
    "    Returns\n",
    "    -------\n",
    "    metric_dict : is a dictionary returned by this function with below keys\n",
    "    train_pred_prob : numpy array containing predicted probability for train set\n",
    "    test_pred_prob : numpy array containing predicted probability for test set\n",
    "    train_pred_class : numpy array containing predicted class for train set\n",
    "    test_pred_class : numpy array containing predicted class for train set\n",
    "    perf_df : Pandas dataframe containing roc_auc,precision,recall,accuracy,TN,FP,FN,TP,gini & ks\n",
    "    train_cf : Confusion matrix for train set\n",
    "    test_cf : Confusion matrix for test set\n",
    "    train_gini_table : Gini Table for training set using kanishk_utils\n",
    "    test_gini_table : Gini Table for test set with train bins using kanishk_utils\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score,precision_score,recall_score,roc_auc_score,f1_score,confusion_matrix\n",
    "    train_pred_prob = estimator.predict_proba(X_train)[:,1]\n",
    "    test_pred_prob = estimator.predict_proba(X_test)[:,1]\n",
    "\n",
    "    train_pred_class = 1*(train_pred_prob > th) #estimator.predict(X_train)\n",
    "    test_pred_class = 1*(test_pred_prob > th) #estimator.predict(X_test)\n",
    "\n",
    "    TN_Train, FP_Train, FN_Train, TP_Train = confusion_matrix(y_train,train_pred_class).ravel()\n",
    "    TN_Test, FP_Test, FN_Test, TP_Test = confusion_matrix(y_test,test_pred_class).ravel()\n",
    "\n",
    "    train_gini_table, train_KS, train_Gini, train_bins = \\\n",
    "    proba_score_performance(actual = pd.Series(y_train), prediction = train_pred_prob, test_set=False,train_bins=0,bins=bins)\n",
    "    test_gini_table, test_KS, test_Gini = \\\n",
    "    proba_score_performance(actual = pd.Series(y_test), prediction = test_pred_prob, test_set=True,train_bins=train_bins,bins=bins)\n",
    "\n",
    "    perf_list = []\n",
    "    perf_list.append(['gini',train_Gini,test_Gini])\n",
    "    perf_list.append(['ks',train_KS,test_KS])\n",
    "    perf_list.append(['roc_auc',roc_auc_score(y_train,train_pred_prob),roc_auc_score(y_test,test_pred_prob)])\n",
    "    perf_list.append(['precision',precision_score(y_train,train_pred_class),precision_score(y_test,test_pred_class)])\n",
    "    perf_list.append(['recall', recall_score(y_train,train_pred_class),recall_score(y_test,test_pred_class)])\n",
    "    perf_list.append(['f1_score',f1_score(y_train,train_pred_class),f1_score(y_test,test_pred_class)])\n",
    "    perf_list.append(['accuracy',accuracy_score(y_train,train_pred_class),accuracy_score(y_test,test_pred_class)])\n",
    "    perf_list.append(['TN',TN_Train,TN_Test])\n",
    "    perf_list.append(['FP',FP_Train,FP_Test])\n",
    "    perf_list.append(['FN',FN_Train,FN_Test])\n",
    "    perf_list.append(['TP',TP_Train,TP_Test])\n",
    "    perf_list = pd.DataFrame(perf_list,columns = ['Metric','Train', 'Test']).set_index('Metric')\n",
    "    train_cf = pd.DataFrame(perf_list.loc[['TN','FP','FN','TP'],'Train'].values.reshape(2,2),\\\n",
    "    index=pd.MultiIndex.from_tuples([('Actual','0'), ('Actual', '1')]),\n",
    "    columns=pd.MultiIndex.from_tuples([('Predicted','0'), ('Predicted', '1')]))\n",
    "    test_cf = pd.DataFrame(perf_list.loc[['TN','FP','FN','TP'],'Test'].values.reshape(2,2),\\\n",
    "    index=pd.MultiIndex.from_tuples([('Actual','0'), ('Actual', '1')]),\n",
    "    columns=pd.MultiIndex.from_tuples([('Predicted','0'), ('Predicted', '1')]))\n",
    "    metric_dict = {'train_pred_prob':train_pred_prob, 'test_pred_prob' : test_pred_prob,\n",
    "    'train_pred_class' : train_pred_class,'test_pred_class': test_pred_class,\n",
    "    'perf_df': perf_list.T, 'train_cf' : train_cf, 'test_cf' :test_cf,\n",
    "    'train_gini_table' : train_gini_table, 'test_gini_table':test_gini_table}\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get colors for max value in a variable or column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colors_maxValue(ds,var_name='KS_Stat'):\n",
    "    '''\n",
    "    This functions highlights the max value cell in var_name\n",
    "    param:\n",
    "        ds : dataframe\n",
    "        var_name : varibale name where highlighting needs to be done\n",
    "    '''\n",
    "    #copy df to new - original data are not changed\n",
    "    df = ds.copy()\n",
    "    df[var_name] = df[var_name].astype(float)\n",
    "    idx= df[var_name].idxmax()\n",
    "    df.loc[:,:] = 'background-color: ' # select all values to default value - no color\n",
    "    df.loc[idx, var_name] = 'background-color: yellow' # overwrite values with other color\n",
    "    #return color df\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_data_with_prob(smry_dict ,ds , sampleType = 'test'):\n",
    "    \"\"\"\n",
    "    get prob attached to woe data for train/test dataset\n",
    "    \n",
    "    smry_dict : dict output of get_perf_metric function\n",
    "    ds : train or test df used for model building\n",
    "    sampleType : str, can take two values : train , test\n",
    "    \"\"\"\n",
    "    pred_dt = pd.concat([pd.Series(smry_dict[str(sampleType)+'_pred_prob']),pd.Series(smry_dict[str(sampleType)+'_pred_class'])],axis=1)\n",
    "    pred_dt.columns = ['pred_prob','pred_class']\n",
    "    pred_dt['key'] = ds.index\n",
    "    print('shape of predicted '+str(sampleType)+' sample: '+str(pred_dt.shape))\n",
    "    \n",
    "    ds['key'] = ds.index\n",
    "    print('shape of '+str(sampleType)+' sample: '+str(ds.shape))\n",
    "    pred_dt = pd.merge(pred_dt,ds,on='key',how='outer')\n",
    "    pred_dt['sample_type'] = sampleType.capitalize()\n",
    "    print('shape post merge of ' + str(sampleType)+' sample: ' +str(pred_dt.shape),'\\n')\n",
    "    \n",
    "    return(pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ScBand_limits_clean(x):\n",
    "    x = re.sub(\"\\(,\",\"\",x) #replace basic\n",
    "    x = re.sub(\"[\\\"]\",\"|\",x) # put pipe seperator\n",
    "    x = re.sub('[(]','',re.sub('[)]','',x)) #removing open '{}'\n",
    "    x = re.sub('[]]','',re.sub('[]]','',x)) #removing open '{}'\n",
    "#     x = re.sub('[|]+','',x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_mapping(modelBase,predProb ,bandProb_ds):\n",
    "    '''\n",
    "    This function tags the band based on train data\n",
    "    \n",
    "    param :\n",
    "        modelBase : model dataframe \n",
    "        predProb : str, predicted_prob feature name\n",
    "        bandProb_ds : dataframe with traning smry dict, contains min, max range of probablities for each brand\n",
    "        \n",
    "    return:\n",
    "        dataframe with band tag\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    var2=[]\n",
    "    var3 =[]\n",
    "    cutoff_pts = bandProb_ds['MAX_VALUE']\n",
    "    for i in range(0,len(modelBase)):\n",
    "        var1 = []\n",
    "        for j in range(0,len(cutoff_pts)):\n",
    "            if (modelBase[predProb][i] < cutoff_pts[j]):\n",
    "                var1.append(j)\n",
    "                if len(var1)==0:\n",
    "                    print(modelBase[predProb][i])\n",
    "                    print(cutoff_pts[j])\n",
    "                    \n",
    "\n",
    "        if len(var1)==0:\n",
    "            var1 = []\n",
    "        else:\n",
    "            var1 = min(var1)\n",
    "        var3.append(var1)\n",
    "        var2.append(i)\n",
    "\n",
    "    ds = pd.DataFrame({'idx':var2,'band':var3})\n",
    "    ds['key'] = modelBase['key']\n",
    "    ds['band'] = ds['band'] + 1\n",
    "    return(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_prob_decile_df(smry_dict,X_train,X_test,model_base_data):\n",
    "    \n",
    "    '''\n",
    "    This function returns the df with predicted prob and scorebands\n",
    "    param :\n",
    "        smry_dict : dict output of get_perf_metric function\n",
    "        X_train : train dataset\n",
    "        X_test : test dataset\n",
    "        model_base_data : base data should have basic indentifiers such as unique id etc\n",
    "    returns :\n",
    "        dataframe with predicted prob\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #- get predicted probabilities\n",
    "    train_pred_df = get_base_data_with_prob(smry_dict,ds = X_train,sampleType = 'train')\n",
    "    test_pred_df = get_base_data_with_prob(smry_dict,ds = X_test,sampleType = 'test')\n",
    "    pred_df = pd.concat([train_pred_df,test_pred_df],axis=0)\n",
    "    print('train predicted df shape: '+str(train_pred_df.shape) +'\\n'+ 'test predicted df shape: '+str(test_pred_df.shape) +'\\n'+ 'overall df shape: '+str(pred_df.shape)+'\\n')\n",
    "    \n",
    "    # Get index from base data to merge the prob from predicted values\n",
    "    model_base_data_act = model_base_data.copy()\n",
    "    print('model base shape:-' + str(model_base_data_act.shape))\n",
    "    model_base_data_act['key'] = model_base_data_act.index\n",
    "    \n",
    "    # merge \n",
    "    model_dev_pred_df = pd.merge(pred_df[['key','pred_prob','pred_class','sample_type']],model_base_data_act,on='key',how='outer')\n",
    "    model_dev_pred_df = model_dev_pred_df.drop_duplicates()\n",
    "    print('pobability df shape:-' + str(pred_df.shape)+'\\n'+  'model base shape post addition of key:- ' + str(model_base_data_act.shape) +'\\n' + 'shape of dataframe post probabilty addition:- ' + str(model_dev_pred_df.shape)+'\\n')\n",
    "    \n",
    "    # getting cutoff values of each scorebands\n",
    "    prob_band_ds = smry_dict['train_gini_table'][smry_dict['train_gini_table']['Probability_of_event'] !='All']\n",
    "    prob_band_ds['Probability_of_event'] = prob_band_ds['Probability_of_event'].astype(str).str.replace('/',' ')\n",
    "    prob_band_ds['MIN_VALUE'] = prob_band_ds[\"Probability_of_event\"].map(make_ScBand_limits_clean).str.split(',').str[-2].str.strip().astype(float)\n",
    "    prob_band_ds['MAX_VALUE'] = prob_band_ds[\"Probability_of_event\"].map(make_ScBand_limits_clean).str.split(',').str[-1].str.strip().astype(float)\n",
    "    prob_band_ds = prob_band_ds[['MIN_VALUE','MAX_VALUE','Band']]\n",
    "    cutoff_pts = prob_band_ds['MAX_VALUE']\n",
    "    \n",
    "    # map the bands the to base data\n",
    "    band_mapping_df = band_mapping(modelBase = model_dev_pred_df,predProb = 'pred_prob' ,bandProb_ds = prob_band_ds)\n",
    "    print(\"Band mapping df shape:- \" + str(band_mapping_df.shape))\n",
    "    band_mapping_df = band_mapping_df.drop(['idx'],axis=1)\n",
    "    \n",
    "    model_dev_pred_df =pd.merge(model_dev_pred_df,band_mapping_df,on='key',how='outer')\n",
    "    print(\"final df shape:- \" + str(model_dev_pred_df.shape))\n",
    "    \n",
    "    return model_dev_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oot_pred_df(estimator,ds,smry_dict):\n",
    "    \n",
    "    '''\n",
    "    This function returns the predicted prob with the band tag of traning  \n",
    "    param :\n",
    "        estimator : fitted model\n",
    "        ds : out of time dataframe \n",
    "        smry_dict : dict output of get_perf_metric function\n",
    "    returns :\n",
    "        dataframe with band mapped\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    oot_pred_df = pd.DataFrame(estimator.predict_proba(ds)[:,1]).rename(columns={0:'predicted_prob'})\n",
    "    ds = pd.concat([ds,oot_pred_df ],axis=1)\n",
    "    ds['key'] = ds.index\n",
    "    \n",
    "    prob_band_ds = smry_dict['train_gini_table'][smry_dict['train_gini_table']['Probability_of_event'] !='All']\n",
    "    prob_band_ds['Probability_of_event'] = prob_band_ds['Probability_of_event'].astype(str).str.replace('/',' ')\n",
    "    prob_band_ds['MIN_VALUE'] = prob_band_ds[\"Probability_of_event\"].map(make_ScBand_limits_clean).str.split(',').str[-2].str.strip().astype(float)\n",
    "    prob_band_ds['MAX_VALUE'] = prob_band_ds[\"Probability_of_event\"].map(make_ScBand_limits_clean).str.split(',').str[-1].str.strip().astype(float)\n",
    "    prob_band_ds = prob_band_ds[['MIN_VALUE','MAX_VALUE','Band']]\n",
    "    \n",
    "    oot_band_mapped_df = band_mapping(modelBase = ds,predProb = 'predicted_prob' ,bandProb_ds = prob_band_ds)\n",
    "    ds =pd.merge(ds,oot_band_mapped_df,on='key',how='outer')\n",
    "    ds = ds.drop(['key','idx'],axis=1)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get model metrics summary in one sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_biz_metrics(model_df):\n",
    "    metrics=model_df[['Cohort','Gini%','KS%','Roc_Auc%','PSI']]\n",
    "    Gini=metrics[['Cohort','Gini%']].reset_index(drop=True)\n",
    "    ks=metrics[['Cohort','KS%']].reset_index(drop=True)\n",
    "    roc=metrics[['Cohort','Roc_Auc%']].reset_index(drop=True)\n",
    "    psi=metrics[['Cohort','PSI']].reset_index(drop=True)\n",
    "    biz_matrics=model_df[['Cohort','nongrey_paybhev_&_score_avail','Event','PPMT','Event Rate%','PPMT Rate%']]\n",
    "    biz_matrics.columns=['Cohort','cx','Target','PPMT','Target_Rate_%','PPMT_Rate_%']\n",
    "    biz_matrics['POP_DIFF_%']=np.round((biz_matrics['cx']/biz_matrics['cx'].shift(1))*100,2)\n",
    "    \n",
    "    for i in range(Gini.shape[0]):\n",
    "        s=Gini.shape[1]-1\n",
    "        Gini[Gini['Cohort'][i]]=[np.round((Gini['Gini%'].loc[i]/Gini['Gini%'].loc[j])*100,2) if j<s else np.nan for j in range(Gini.shape[0])]\n",
    "    Gini.loc[Gini.shape[0]]=np.nan\n",
    "    Gini.loc[Gini.shape[0]]=np.nan    \n",
    "    \n",
    "    for i in range(ks.shape[0]):\n",
    "        s=ks.shape[1]-1\n",
    "        ks[ks['Cohort'][i]]=[np.round((ks['KS%'].loc[i]/ks['KS%'].loc[j])*100,2) if j<s else np.nan for j in range(ks.shape[0])]\n",
    "    ks.loc[ks.shape[0]]=np.nan\n",
    "    ks.loc[ks.shape[0]]=np.nan   \n",
    "\n",
    "    \n",
    "    for i in range(roc.shape[0]):\n",
    "        s=roc.shape[1]-1\n",
    "        roc[roc['Cohort'][i]]=[np.round((roc['Roc_Auc%'].loc[i]/roc['Roc_Auc%'].loc[j])*100,2) if j<s else np.nan for j in range(roc.shape[0])]\n",
    "    roc.loc[roc.shape[0]]=np.nan\n",
    "    roc.loc[roc.shape[0]]=np.nan\n",
    "    \n",
    "    for i in range(psi.shape[0]):\n",
    "        s=psi.shape[1]-1\n",
    "        psi[psi['Cohort'][i]]=[np.round((psi['PSI'].loc[i]/psi['PSI'].loc[j])*100,2) if j<s else np.nan for j in range(psi.shape[0])] \n",
    "        psi.loc[0]=-10\n",
    "        psi.loc[0,'Cohort']='Development'\n",
    "    \n",
    "    Gini.loc[Gini.shape[0]]=ks.columns\n",
    "    ks.loc[ks.shape[0]]=roc.columns\n",
    "    roc.loc[ks.shape[0]]=psi.columns\n",
    "    Gini=Gini.rename(columns={'Gini%':'Gini'})\n",
    "    ks=ks.rename(columns={'KS%':'Gini'})\n",
    "    roc=roc.rename(columns={'Roc_Auc%':'Gini'})  \n",
    "    psi=psi.rename(columns={'PSI':'Gini'})\n",
    "    model_metric=pd.concat([Gini,ks,roc,psi],axis=0) \n",
    "    \n",
    "    \n",
    "    return model_metric,biz_matrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to fetch data from MYsql "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch Cohort data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_lst(df_size,batch_size):\n",
    "    return [i*batch_size for i in range(int(df_size/batch_size)+2)]\n",
    "\n",
    "def get_sql_conn(user_id, password):\n",
    "    from sqlalchemy import create_engine\n",
    "    from urllib.parse import quote      \n",
    "    server = create_engine('''mysql+pymysql://{0}:{1}s@10.95.60.125:3306/ypre'''.format(user_id,'%') % quote(password))\n",
    "    return server\n",
    "def header(name):\n",
    "#     colorstr = \"\"\"<h4><center>{}</center></h4>\"\"\".format(name)\n",
    "#     display(Markdown(colorstr))\n",
    "    print(name)\n",
    "\n",
    "def preprocess_cohort_data(df_):\n",
    "    print('Data preprocessing starts')\n",
    "    df=df_.copy()\n",
    "    df['due_month'] = pd.to_datetime(df['emi_due_date']).apply(lambda x: x.strftime(\"%Y%m\") if pd.isna(x)!=True else np.nan)\n",
    "    df['disbursed_month'] = pd.to_datetime(df['disbursed_date']).apply(lambda x: x.strftime(\"%Y%m\") if pd.isna(x)!=True else np.nan)\n",
    "    df['source_month'] = pd.to_datetime(df['createdOn']).apply(lambda x: x.strftime(\"%Y%m\") if pd.isna(x)!=True else np.nan)\n",
    "    df['ppmt'] = np.where(df['delay']<=0,1,0)\n",
    "    header('Null Summary')\n",
    "    null_smry=df[['due_month','disbursed_month','source_month','ppmt','delay']].isnull().sum().reset_index()\n",
    "    null_smry.columns=['Column Names','# of Null Rows']\n",
    "    display(null_smry)\n",
    "    print('End of data preprocessing')\n",
    "    return df\n",
    "def header_left(name):\n",
    "    print(name)\n",
    "def header_left(name):\n",
    "    print(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohort_fetch(start_date,end_date,sql_userid='',sql_password=''):\n",
    "    server=get_sql_conn(sql_userid, sql_password)\n",
    "    if(server==None):\n",
    "        return 400\n",
    "    query=f''' select \n",
    "    userid as cid,product_name,delay,createdOn, disbursed_date, emi_due_date \n",
    "    from ypdynamic.yp_emi_data_tbl \n",
    "\n",
    "    where emi_due_date >= \"{start_date}\" and emi_due_date <\"{end_date}\" and loan_number=1 and installment_number=1\n",
    "   ;\n",
    "    '''\n",
    "    data=pd.read_sql(query, server)\n",
    "    print(f'Data Fetch completed. Shape of data fetch: {data.shape}')\n",
    "    server.dispose()\n",
    "    data=preprocess_cohort_data(data)\n",
    "    print(f'Final shape of data after preprocessing: {data.shape}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch syncId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sync_loan_application_v1(df_, conn):\n",
    "    ''' This function takes userIds/cid & loanIds to fetch syncIds from yp.yp_user_sync_data based on loan_application logic.\n",
    "    This code runs for one batch.\n",
    "    Input: \n",
    "    userIds : list of customer ids, list\n",
    "    loanIds : list of loan ids, list\n",
    "    batch_size : Batch size, int >0 & should be in multiples of 10,100,1000,10000 and so on.....\n",
    "    conn= SQL create_engine statement, e.g. create_engine('-------conn_string----------') \n",
    "    \n",
    "    Output:\n",
    "    DataFrame containing userid & syncid.\n",
    "    '''\n",
    "    if(conn==None ):\n",
    "        print('Check connection string or length of userids/oanids passed.')\n",
    "        return 400\n",
    "    import datetime as dt\n",
    "    user_lst=list(df_.userid.values.tolist()) \n",
    "    loanId_lst=list(df_.loan_id.values.tolist())\n",
    "    sync_batch_query='''select cid as userid, max(syncId) as syncId,appType\n",
    "                from yp.yp_user_sync_data\n",
    "                where cid in {0} and source = 'loan_application' and sourceId in {1} \n",
    "                group by cid;'''  ## Query to fetch syncId based on loan_application logic\n",
    "    print(f'Starting Fetch Logic: Loan Application')\n",
    "    t1=dt.datetime.now()\n",
    "    temp_query=sync_batch_query.format(tuple(user_lst),tuple(loanId_lst))\n",
    "    temp_data= pd.read_sql(temp_query, conn)\n",
    "    temp_data['fetch_logic']='loan_application'\n",
    "    print('Fetch Done')\n",
    "    elapsed=dt.datetime.now()-t1\n",
    "    print(\"df shape: \" + str(temp_data.shape) + \"Took H:M:S - : %02d:%02d:%02d\" % (elapsed.seconds // 3600, elapsed.seconds // 60 % 60, elapsed.seconds % 60))\n",
    "    return temp_data\n",
    "\n",
    "def run_sync_7day_logic_v1(df_,conn):\n",
    "    ''' This function takes dataframe containig userid & disburse_date to fetch syncIds from yp.yp_user_sync_data based on Approximation_7d_lag_from_disb_date logic.\n",
    "    This code runs for one batch.\n",
    "    Input: \n",
    "    df_ : dataframe containig userid & disburse_date\n",
    "    batch_size : Batch size, int >0 & should be in multiples of 10,100,1000,10000 and so on.....\n",
    "    conn= SQL create_engine statement, e.g. create_engine('-------conn_string----------') \n",
    "    \n",
    "    Output:\n",
    "    DataFrame containing userid & syncid.\n",
    "    '''\n",
    "    if(conn==None):\n",
    "        return 400\n",
    "    import datetime as dt\n",
    "    df=df_.copy()\n",
    "    df.disbursed_date=df.disbursed_date.astype('str')    # Converting disbursed date into string format. This will be used to create a temp table for filteration of data\n",
    "    user_lst=list(df.userid.values.tolist())             \n",
    "    sync_7day_query='''with base_data as (select column_0 as cid, column_1 as disbursed_date from ( values {0} ) as temp) \n",
    "\n",
    "            select t1.cid as userid, max(t1.syncId) as syncId,appType\n",
    "            from yp.yp_user_sync_data t1\n",
    "            join base_data t2 on t1.cid in {1} and t1.cid = t2.cid and \n",
    "            t1.updatedAt>= DATE_ADD(t2.disbursed_date, INTERVAL -7 DAY) and t1.updatedAt<=t2.disbursed_date\n",
    "            and source!='session'\n",
    "            group by 1;'''                     # Query to fetch syncId based on 7 day logic \n",
    "\n",
    "    print(f'Starting Fetch Logic: Approximation_7d_lag_from_disb_date')\n",
    "    t1=dt.datetime.now()\n",
    "    temp_table_data=','.join(['row'+str(tuple(i)) for i in df[df.userid.isin(user_lst)][['userid','disbursed_date']].values])   # data creation for temp table (table created in select statement)\n",
    "    temp_query=sync_7day_query.format(temp_table_data,tuple(user_lst))\n",
    "    temp_data= pd.read_sql(temp_query, conn)\n",
    "    temp_data['fetch_logic']='Approximation_7d_lag_from_disb_date'\n",
    "    print('Fetch Done')\n",
    "    elapsed=dt.datetime.now()-t1\n",
    "    print(\"df shape: \" + str(temp_data.shape) + \"Took H:M:S - : %02d:%02d:%02d\" % (elapsed.seconds // 3600, elapsed.seconds // 60 % 60, elapsed.seconds % 60))\n",
    "    return temp_data\n",
    "             \n",
    "\n",
    "def fetch_sync_v1(df_,batch_size=10000,sql_userid='',sql_password=''):\n",
    "    import datetime as dt\n",
    "    ''' This function takes dataframe containig atleast userid,loanid & disburse_date columns to fetch syncIds from yp.yp_user_sync_data based on loan_application & 7days from disburse date logic.\n",
    "    This code runs in batchwise mode.\n",
    "    Input: \n",
    "    df_          : dataframe containig atleast userid,loanid & disburse_date columns\n",
    "    batch_size   : Batch size, int >0 & should be in multiples of 10,100,1000,10000 and so on.....\n",
    "    sql_userid   : SQL user_id, default ='' No params i.e. params stored in credential.txt\n",
    "    sql_password : SQL password, default ='' No params i.e. params stored in credential.txt\n",
    "    \n",
    "    Output:\n",
    "    DataFrame containing all existing columns along with syncid.\n",
    "    '''\n",
    "    req_cols=['userid','loan_id','disbursed_date']\n",
    "    header('-'*5+'Fetching SyncIds for Data'+'-'*5)\n",
    "    df=df_.copy()\n",
    "    server=get_sql_conn(sql_userid,sql_password)    # Getting SQL create_engine statement\n",
    "    if(server==None):\n",
    "        return 400\n",
    "    if(not all([i in df.columns for i in req_cols])):\n",
    "        header(' Column Error ')\n",
    "        print(f'Please make sure all required columns : {req_cols} are present in data & with same name.')\n",
    "        return 400\n",
    "    try:\n",
    "        final_data=pd.DataFrame()\n",
    "        user_lst=df.userid.values.tolist()\n",
    "       \n",
    "        shape_lst=get_shape_lst(len(user_lst),batch_size)  # getting shape list based on size of data & batch_run_size\n",
    "\n",
    "        for i in range(len(shape_lst)-1):\n",
    "     \n",
    "            header_left(f'Start : End - {shape_lst[i]}:{shape_lst[i+1]}')\n",
    "            sub_df=df[df.userid.isin(user_lst[shape_lst[i]:shape_lst[i+1]])][req_cols]\n",
    "           \n",
    "            sync_loanApp=run_sync_loan_application_v1(sub_df, conn=server)    # Fetching sync based on loan_application _logic\n",
    "\n",
    "            null_sync_df=sub_df[~sub_df.userid.isin(sync_loanApp.userid.tolist())]  \n",
    "              # filteration of data where sync is null\n",
    "            sync_7day=run_sync_7day_logic_v1(null_sync_df[['userid','disbursed_date']],conn=server) # Fetching sync based on Approximation_7d_lag_from_disb_date _logic\n",
    "    \n",
    "            sub_df=sub_df.merge(sync_loanApp.append(sync_7day),on='userid',how='left')\n",
    "            final_data=final_data.append(sub_df)\n",
    "            print(f'\\n{sub_df[sub_df.syncId.notnull()].shape[0]}/{sub_df.shape[0]} , {round(100*sub_df[sub_df.syncId.notnull()].shape[0]/sub_df.shape[0],2)}% syncIds found.\\n')\n",
    "            \n",
    "        print('End of Batch Execution.')\n",
    "    except Exception as e:\n",
    "        header('ERROR Occured')\n",
    "        print(f'Error : {e}')\n",
    "    if(final_data.shape[0]>0):\n",
    "        print(f'\\nTotal {final_data[final_data.syncId.notnull()].shape[0]}/{final_data.shape[0]},{round(100*final_data[final_data.syncId.notnull()].shape[0]/final_data.shape[0],2)} syncIds found.')\n",
    "        header('-'*5+'Fetch Sync Complete.'+'-'*5)\n",
    "        return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6209b9ff55fbb8ac2244490389c18204ec5eca53750a40636b48eaae1101e508"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
